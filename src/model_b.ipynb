{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7450d71-b3fb-4acb-8715-73ce35f9b9fc",
   "metadata": {},
   "source": [
    "<!-- PROJECT LOGO -->\n",
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"https://res.cloudinary.com/dek4evg4t/image/upload/v1729273000/Group_4.png\" alt=\"Logo\" width=\"30%\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "### 🖹 Descripción:\n",
    "Este Proyecto tiene como objetivo aplicar redes neuronales convolucionales (CNN) para realizar una clasificación multiclase de imágenes mediante aprendizaje supervisado. Utilizando el [Covid-19 Image Dataset de Kaggle](https://www.kaggle.com/datasets/pranavraikokte/covid19-image-dataset), que contiene imágenes de rayos X clasificadas en tres categorías (Covid-19, Normal, Neumonía), en este proyecto se desarrollarán clasificadores capaces de diagnosticar enfermedades pulmonares. El proyecto también explora el uso de PyTorch para el desarrollo de modelos de Machine Learning y herramientas de monitoreo, como Weights and Biases, para el seguimiento en tiempo real del proceso de entrenamiento.\n",
    "\n",
    "### ✍️ Autores:\n",
    "* Angelo Ortiz Vega - [@angelortizv](https://github.com/angelortizv)\n",
    "* Alejandro Campos Abarca - [@MajinLoop](https://github.com/MajinLoop)\n",
    "\n",
    "### 📅 Fecha:\n",
    "20 de octubre de 2024\n",
    "\n",
    "### 📝 Notas:\n",
    "Este es el segundo proyecto del curso IC6200 - Inteligencia Artificial. En este notebook, titulado \"Covid-19 Classification\", se profundiza en técnicas de data augmentation, preprocesamiento de imágenes con filtros, y fine-tuning de modelos CNN como VGG16 para mejorar la capacidad de generalización de las redes neuronales convolucionales.\n",
    "\n",
    "### Otras notas:\n",
    "Asegurarse de contar con Python y las siguientes bibliotecas instaladas: torch, torchvision, cv2, numpy, matplotlib, Pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59076502",
   "metadata": {},
   "source": [
    "# 1. Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb193973-d2be-44d9-9aa2-6b77c6853646",
   "metadata": {},
   "source": [
    "## 1.1 Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecfc9c-01ea-47af-a3cb-907e031dafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from torchviz import make_dot\n",
    "os.environ[\"PATH\"] += r\";C:\\Program Files\\Graphviz\\bin\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0244b6e",
   "metadata": {},
   "source": [
    "## 1.2 Verificar PyThorch y CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c823e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if(torch.cuda.is_available()):\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a7e24-bb26-4dc5-9d85-16c332c33c37",
   "metadata": {},
   "source": [
    "## 1.3 Definición de Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a36b3-41b3-49df-9d28-cc8db2174e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = 153\n",
    "IMAGE_RESIZE = 128\n",
    "IMAGE_NORMALIZE = 0.5\n",
    "\n",
    "HYPERPARAMETERS = \\\n",
    "{\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"dropout_rate\": 0.5,\n",
    "}\n",
    "\n",
    "# Paths\n",
    "TRAIN_DATA_PATH = 'data/Covid19-dataset/train'\n",
    "TEST_DATA_PATH = 'data/Covid19-dataset/test'\n",
    "VISUALIZATION_DIR = 'visualization'\n",
    "MODEL_B_GRAPHS_DIR = 'Model_B_graphs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7afea1",
   "metadata": {},
   "source": [
    "## 1.4 Configuración de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d093e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch settings\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "\n",
    "# Configuración de Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27921004",
   "metadata": {},
   "source": [
    "## 1.5 Definición de filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausian_blur(img, order=21, sigma=21):\n",
    "    np_img = np.array(img)\n",
    "    filtered_img = cv2.GaussianBlur(np_img, (order,order), sigma)\n",
    "    return Image.fromarray(filtered_img)\n",
    "\n",
    "def bilateral_filter(img):\n",
    "    \"\"\"\n",
    "    d: Diameter of each pixel neighborhood.\n",
    "    \n",
    "    sigmaColor: Value of \\sigma in the color space. The greater the value, the colors farther to each other will start to get mixed.\n",
    "    \n",
    "    sigmaSpace: Value of \\sigma in the coordinate space. The greater its value, the more further pixels will mix together, given that\n",
    "    their colors lie within the sigmaColor range.    \n",
    "    \"\"\"\n",
    "    np_img = np.array(img)\n",
    "    filtered_img = cv2.bilateralFilter(np_img, 15, 75, 75)\n",
    "    return Image.fromarray(filtered_img) \n",
    "\n",
    "def canny_edge_filter(img, min_val=40, max_val=85):\n",
    "    img = np.array(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # PIL (RGB) -> OpenCV (BGR)\n",
    "    img = cv2.Canny(img, min_val, max_val)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV (BGR) -> PIL (RGB)\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff29366-f8e8-4e64-93fd-3f7471a65a5e",
   "metadata": {},
   "source": [
    "## 1.6 Creación de los dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d6376",
   "metadata": {},
   "source": [
    "### 1.6.1 Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_r = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_r_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotación aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotación\n",
    "        \n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_b = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Lambda(lambda img: bilateral_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_b_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotación aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotación\n",
    "\n",
    "        transforms.Lambda(lambda img: bilateral_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_c = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Lambda(lambda img: canny_edge_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_c_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotación aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotación\n",
    "\n",
    "        transforms.Lambda(lambda img: canny_edge_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519de310",
   "metadata": {},
   "source": [
    "### 1.6.2 Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482fd7a-4983-42d6-84d0-c950afcee3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_data_loader(dataset, fraction_of_data=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    subset_size = int(np.floor(fraction_of_data * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    sub_dataset = Subset(dataset, subset_indices)\n",
    "    # print(f'Subset size: {len(sub_dataset)}')\n",
    "    return DataLoader(dataset=sub_dataset, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Train data\n",
    "# Raw\n",
    "train_dataset_r = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_r)\n",
    "train_dataset_r_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_r_augmented)\n",
    "combined_train_dataset_r = ConcatDataset([train_dataset_r, train_dataset_r_augmented])\n",
    "train_loader_r = DataLoader(dataset=combined_train_dataset_r, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_r = get_sub_data_loader(dataset=combined_train_dataset_r)\n",
    "\n",
    "# Bilateral\n",
    "train_dataset_b = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_b_augmented)\n",
    "train_dataset_b_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_b_augmented)\n",
    "combined_train_dataset_b = ConcatDataset([train_dataset_b, train_dataset_b_augmented])\n",
    "train_loader_b = DataLoader(dataset=combined_train_dataset_b, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_b = get_sub_data_loader(dataset=combined_train_dataset_b)\n",
    "\n",
    "# Canny\n",
    "train_dataset_c = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_c_augmented)\n",
    "train_dataset_c_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_c_augmented)\n",
    "combined_train_dataset_c = ConcatDataset([train_dataset_c, train_dataset_c_augmented])\n",
    "train_loader_c = DataLoader(dataset=combined_train_dataset_c, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_c = get_sub_data_loader(dataset=combined_train_dataset_c)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Test data\n",
    "# Raw\n",
    "test_dataset_r = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_r_augmented)\n",
    "test_loader_r = DataLoader(dataset=test_dataset_r, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "# Bilateral\n",
    "test_dataset_b = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_b_augmented)\n",
    "test_loader_b = DataLoader(dataset=test_dataset_b, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "# Canny\n",
    "test_dataset_c = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_c_augmented)\n",
    "test_loader_c = DataLoader(dataset=test_dataset_c, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(f'Tamaño del conjunto de entrenamiento: {len(train_loader_r.dataset)}')\n",
    "print(f'Tamaño del conjunto de testing: {len(test_loader_r.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78657128-b115-4cd4-bf2b-1f1e1a70980c",
   "metadata": {},
   "source": [
    "## 1.7 Visualización de los filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde176b-bbef-4d03-a52a-a052f9cbe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_five_images(data_loader, dataset):\n",
    "    \n",
    "    try:\n",
    "        images, labels = next(iter(data_loader))\n",
    "    \n",
    "    except StopIteration:\n",
    "        print(\"El DataLoader no tiene más datos.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error: {e}\")\n",
    "    \n",
    "    images = images.numpy().transpose((0, 2, 3, 1))\n",
    "    images = np.clip(images, 0, 1)\n",
    "\n",
    "    _, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for ax, img, label in zip(axes, images[:5], labels[:5]):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Clase: {dataset.classes[label.item()]}')\n",
    "    plt.show()\n",
    "\n",
    "show_first_five_images(train_loader_r, train_dataset_r)\n",
    "show_first_five_images(train_loader_b, train_dataset_b)\n",
    "show_first_five_images(train_loader_c, train_dataset_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30d06b",
   "metadata": {},
   "source": [
    "Los colores se ven inusuales porque `plt` asume que la imagen está en formato RGB, pero en realidad solo tiene un canal de color. Esto se debe a que el modelo está diseñado para aceptar entradas con solo un canal de color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f7547",
   "metadata": {},
   "source": [
    "## 1.8 Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, model_name):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    initial_wandb_learning_rate = wandb.config.learning_rate # Guardar el learning rate inicial\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "\n",
    "        accumulated_epoch_loss = 0\n",
    "        epoch_correct_predictions = 0\n",
    "        epoch_total_samples = 0\n",
    "\n",
    "        # For batch\n",
    "        for images, labels in train_data_loader:\n",
    "\n",
    "            # Limpiar gradientes: Se restablecen los gradientes acumulados en el optimizador a cero para evitar que se sumen a los gradientes del lote actual.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Se pasan las imágenes a través del modelo para obtener las predicciones (model_predictions).\n",
    "            model_predictions = model(images)\n",
    "\n",
    "            # Cálculo de la pérdida: Se calcula la pérdida utilizando la función de pérdida definida anteriormente,\n",
    "            # comparando las predicciones (model_predictions) con las etiquetas verdaderas (labels).\n",
    "            batch_loss = criterion(model_predictions, labels)\n",
    "            \n",
    "            # Sumar la pérdida de este lote a la pérdida total de la época\n",
    "            accumulated_epoch_loss += batch_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(model_predictions, 1)  # Obtener las predicciones con el mayor valor (clase más probable)\n",
    "            batch_correct_predictions = (predicted == labels).sum().item()    # Contar cuántas predicciones son correctas\n",
    "            total_samples = labels.size(0)                          # Total de elementos en el batch\n",
    "\n",
    "            epoch_correct_predictions += batch_correct_predictions\n",
    "            epoch_total_samples += total_samples\n",
    "\n",
    "            # Backward pass: Se realiza la propagación hacia atrás para calcular los gradientes de la pérdida con respecto a los parámetros del modelo.\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Actualizar parámetros: Se actualizan los parámetros del modelo utilizando los gradientes calculados en la etapa anterior.\n",
    "            optimizer.step()\n",
    "\n",
    "        # Cálculo de la accuracy promedio de la época\n",
    "        epoch_loss = accumulated_epoch_loss / len(train_data_loader)\n",
    "        epoch_accuracy = 100 * epoch_correct_predictions / epoch_total_samples\n",
    "\n",
    "        # Imprimir pérdida promedio y accuracy por época, así como el LR\n",
    "        current_lr = optimizer.param_groups[0]['lr']    # Obtener el learning rate actual\n",
    "        print(f'Epoch [{epoch + 1}/{wandb.config.epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # WandB\n",
    "        wandb.config.update({\"learning_rate\": current_lr})\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": epoch_loss, \"accuracy\": epoch_accuracy, \"model\": model_name}, commit=True)\n",
    "\n",
    "        scheduler.step(epoch_loss)\n",
    "    \n",
    "    # Restaurar el valor inicial de learning rate después del entrenamiento\n",
    "    wandb.config.update({\"learning_rate\": initial_wandb_learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6d846",
   "metadata": {},
   "source": [
    "- **mode**='min': El scheduler se activará si la pérdida se encuentra en su mínimo.\n",
    "- **factor**=0.5: Reducirá el learning rate a la mitad.\n",
    "- **patience**=2: Esperará 2 épocas antes de reducir el learning rate si no hay mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfa191",
   "metadata": {},
   "source": [
    "## 1.9 Funciones de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b315aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, labels in test_data_loader:\n",
    "\n",
    "            model_predictions = model(images)\n",
    "            _, predicted = torch.max(model_predictions.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_wandb_project(\n",
    "        project_name: str,\n",
    "        model_name: str,\n",
    "        hyperparameters: dict,\n",
    "        model,\n",
    "        print_msg: str,\n",
    "        data_loader: DataLoader,\n",
    "        wandb_watch_log = \"all\"):\n",
    "    \n",
    "    wandb.init(project=project_name, name=model_name)\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.watch(model, log=wandb_watch_log)\n",
    "    print(print_msg)\n",
    "    train(model=model, train_data_loader=data_loader, model_name=model)\n",
    "    print()\n",
    "    wandb.unwatch(model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255d354",
   "metadata": {},
   "source": [
    "# 2. Modelo B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727ecae",
   "metadata": {},
   "source": [
    "## 2.1 Arquitectura B-A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f7da2",
   "metadata": {},
   "source": [
    "### 2.1.1 Definición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e67a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=(128 * 16 * 16), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN1.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.pool(self.inception1(x))       # [32, 32, 64, 64] -incep-> [32, 128, 64, 64] -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 32, 32] -conv-> [32, 128, 32, 32] -ReLu-> same -pooling-> [32, 128, 16, 16]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 128 * 16 * 16)           # [32, 128, 16, 16] -view-> [32, 128*16*16]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 128*16*16] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01b71e",
   "metadata": {},
   "source": [
    "#### Justificación de la arquitectura B-A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958726f",
   "metadata": {},
   "source": [
    "La arquitectura propuesta para esta CNN está diseñada de manera que logre un equilibrio entre simplicidad y capacidad de extracción de características en imágenes. A continuación, justifico las decisiones tomadas en la estructura del modelo:\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "Se han utilizado tres capas convolucionales con un número creciente de filtros (32, 128) y un tamaño de kernel de 3x3, junto con un bloque Inception. Esta configuración permite:\n",
    "- **Extracción progresiva de características**: Al aumentar el número de canales, la red puede capturar detalles de baja a alta complejidad a medida que las imágenes pasan por las capas.\n",
    "- **Receptive field**: El tamaño de 3x3 es un estándar eficaz que equilibra precisión y costo computacional, permitiendo que la red capture patrones locales en las imágenes.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "La incorporación de un bloque Inception permite que la red capture una variedad más amplia de características a diferentes escalas. Las ventajas incluyen:\n",
    "- **Múltiples tamaños de convolución**: El bloque utiliza convoluciones con diferentes tamaños de kernel (1x1, 3x3, 5x5) en paralelo, permitiendo que la red aprenda a detectar características en diversas escalas.\n",
    "- **Reducción de dimensionalidad**: Las convoluciones de 1x1 ayudan a reducir el número de canales, optimizando la carga computacional.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "El uso de `MaxPool2D` con kernel 2x2 reduce las dimensiones espaciales de las imágenes (de 128x128 a 16x16 tras 3 pasos), permitiendo:\n",
    "- **Reducción de la dimensionalidad**: Lo cual es clave para disminuir el número de parámetros en las capas completamente conectadas (fully connected), evitando el sobreajuste.\n",
    "- **Retención de información importante**: El max pooling retiene los valores máximos en cada región, preservando características clave mientras descarta información redundante.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "Se ha elegido un número moderado de neuronas en las capas `fc1` (64) y `fc2` (3), con las siguientes razones:\n",
    "- **Compactación de las características**: La salida de la última capa convolucional se transforma en un vector de características mediante `view`, y las capas fully connected resumen esta información en un formato útil para la clasificación.\n",
    "- **Reducción final a 3 clases**: La capa `fc2` genera una salida de 3 neuronas, correspondiente a las 3 categorías del problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "Se introduce dropout en las últimas iteraciones con el fin de:\n",
    "- **Evitar el sobreajuste**: Dropout es una técnica eficaz para mejorar la generalización del modelo, ya que previene la dependencia excesiva en ciertos patrones durante el entrenamiento.\n",
    "\n",
    "##### 6. **Resultados experimentales**\n",
    "El desempeño del modelo mejora notablemente a lo largo de las iteraciones al ajustar los hiperparámetros (learning rate, epochs) y al incluir dropout. Esto valida que la arquitectura es capaz de aprender representaciones útiles cuando se le da suficiente tiempo de entrenamiento y se aplican técnicas de regularización adecuadas.\n",
    "\n",
    "##### 7. **Etapas**\n",
    "Las **etapas** de esta red CNN se pueden dividir en tres bloques principales:\n",
    "\n",
    "###### 1. **Etapa de extracción de características**\n",
    "   - **Capas**: `conv1`, `inception1`, `conv2`, y `pool`\n",
    "   - **Descripción**: Esta etapa utiliza capas convolucionales y de pooling para extraer características de la imagen de entrada. La activación `ReLU` añade no linealidad y el `MaxPool2D` reduce la dimensionalidad de los mapas de características.\n",
    "     - Conv1: Extrae características iniciales de la imagen (bordes, texturas).\n",
    "     - Inception1: Captura características en múltiples escalas.\n",
    "     - Conv2: Extrae aún más características complejas.\n",
    "     - MaxPool: Reduce la dimensión espacial después de cada convolución.\n",
    "\n",
    "###### 2. **Etapa de aplanamiento (Flattening)**\n",
    "   - **Capas**: `x.view(-1, 128 * 16 * 16)`\n",
    "   - **Descripción**: Aplana la salida de la última capa convolucional en un vector para poder pasarla a las capas totalmente conectadas.\n",
    "\n",
    "###### 3. **Etapa de clasificación**\n",
    "   - **Capas**: `fc1`, `dropout`, `fc2`\n",
    "   - **Descripción**: Las capas completamente conectadas (`fc1` y `fc2`) se encargan de la clasificación final. `fc1` reduce la dimensionalidad a 64 neuronas, y después se aplica un `dropout` para prevenir el sobreajuste. Finalmente, `fc2` produce la salida de 3 clases (asumiendo una clasificación de 3 categorías).\n",
    "\n",
    "Estas tres etapas forman la estructura completa de la CNN.\n",
    "\n",
    "##### Transformación de `x` a través de la red\n",
    "El tensor de entrada `x` tiene la forma `[N, C, H, W]`, donde:\n",
    "- **N** = Batch size (número de muestras).\n",
    "- **C** = Número de canales.\n",
    "- **H** = Altura de la entrada.\n",
    "- **W** = Ancho de la entrada.\n",
    "\n",
    "- **Input shape**: `[32, 1, 128, 128]`\n",
    "1. `[32, 1, 128, 128]` → **Conv1** → `[32, 32, 128, 128]` → **ReLU** → `[32, 32, 128, 128]` → **Pooling** → `[32, 32, 64, 64]`\n",
    "2. `[32, 32, 64, 64]` → **Inception1** → `[32, 128, 64, 64]` → **Pooling** → `[32, 128, 32, 32]`\n",
    "3. `[32, 128, 32, 32]` → **Conv2** → `[32, 128, 32, 32]` → **ReLU** → `[32, 128, 32, 32]` → **Pooling** → `[32, 128, 16, 16]`\n",
    "4. `[32, 128, 16, 16]` → **View** → `[32, 128 * 16 * 16]`\n",
    "5. `[32, 128 * 16 * 16]` → **Fully Connected (fc1)** → `[32, 64]`\n",
    "6. `[32, 64]` → **Fully Connected (fc2)** → `[32, 3]`\n",
    "\n",
    "###### `x.view()` function:\n",
    "La función `view()` en PyTorch se utiliza para cambiar la forma de un tensor sin alterar sus datos subyacentes.\n",
    "Transforma el tensor de entrada de una forma 4D `[N, C, H, W]` a una forma 2D `[N, F]`, donde:\n",
    "- **F** = Número de características (features).\n",
    "El número de características se calcula como:\n",
    "\\[\n",
    "F = C \\times H \\times W\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad7db3",
   "metadata": {},
   "source": [
    "#### Explicación de las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae7096",
   "metadata": {},
   "source": [
    "##### Capas de Pooling\n",
    "Una capa de pooling reduce la dimensionalidad del mapa de características generado por la capa de convolución. Funciona tomando un subconjunto de valores (por ejemplo, el valor máximo o promedio) dentro de una ventana deslizante, lo que disminuye el tamaño de la representación. Esto ayuda a preservar las características más importantes mientras se descartan detalles menos relevantes.\n",
    "La reducción en la dimensionalidad también mejora la eficiencia computacional y ayuda a prevenir el sobreajuste. En resumen, el pooling simplifica la representación y permite que el modelo se enfoque en las características más significativas.\n",
    "\n",
    "###### Cálculo de la Dimensionalidad:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d' = \\frac{d - k}{s} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Parámetros:\n",
    "- **d -> dimension_in**: Es el tamaño de la dimension de entrada\n",
    "- **k -> kernel_size**: Es el tamaño del kernel (matriz de orden $k^2$).\n",
    "- **s -> stride**: El número de píxeles que el filtro se desplaza sobre la imagen de entrada, con un valor de 1 significando que se mueve un píxel a la vez.\n",
    "- **d' -> dimension_out**: Es el tamaño de la dimension de salida.\n",
    "\n",
    "##### Capas de convolución\n",
    "Una capa de convolución extrae características de la imagen de entrada mediante filtros que se deslizan sobre la misma. \n",
    "Aplica una operación matemática llamada convolución, produciendo un mapa de características que resalta patrones como bordes y texturas. \n",
    "Mantiene la estructura espacial de la imagen, permitiendo que el modelo aprenda relaciones espaciales. \n",
    "Generalmente, se le aplica una función de activación (como ReLU) para introducir no linealidades.\n",
    "\n",
    "###### Cálculo de la Dimensionalidad:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d' = \\frac{d + 2p - k}{s} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Parámetros:\n",
    "- **k -> kernel_size**: Es el orden de una matriz cuadrada.\n",
    "- **s -> stride**: El número de píxeles que el filtro se desplaza sobre la imagen de entrada, con un valor de 1 significando que se mueve un píxel a la vez.\n",
    "- **p -> padding**: El número de píxeles añadidos alrededor de la imagen de entrada para mantener las dimensiones de salida. El padding ayuda a prevenir que la imagen se reduzca demasiado después de la convolución, permitiendo una mejor retención de información en los bordes.\n",
    "\n",
    "\n",
    "##### Capas Fully Connected\n",
    "Una capa totalmente conectada (fully connected layer) conecta cada neurona de la capa anterior a cada neurona de la capa actual. Se utiliza para combinar las características extraídas por las capas previas y realizar la clasificación o regresión. \n",
    "Cada neurona en esta capa aplica una transformación lineal a las entradas, seguida de una función de activación, lo que permite al modelo aprender patrones complejos. Las capas totalmente conectadas suelen encontrarse al final de una red neuronal, después de las capas de convolución y pooling.\n",
    "En resumen, estas capas integran la información y producen la salida final del modelo.\n",
    "\n",
    "\n",
    "##### Capa Dropout\n",
    "La capa `Dropout` es una técnica de regularización utilizada para prevenir el sobreajuste durante el entrenamiento de redes neuronales. En cada paso de entrenamiento, desactiva aleatoriamente un porcentaje de neuronas (definido por `DROPOUT_RATE`), lo que obliga a la red a no depender demasiado de una sola neurona. Esto mejora la generalización del modelo, ayudando a que aprenda patrones más robustos. En esta CNN, se aplica después de la primera capa completamente conectada para mejorar el rendimiento en tareas de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0751-fc07-4e8d-8490-65c04f45857e",
   "metadata": {},
   "source": [
    "### 2.1.2 Creación de los modelos B-A1 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef183fe-154c-4f86-a5d3-5fe294692448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a1_r = CNN1()  # Modelo para datos crudos\n",
    "model_b_a1_b = CNN1()  # Modelo para datos con filtro bilateral\n",
    "model_b_a1_c = CNN1()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86faecb3",
   "metadata": {},
   "source": [
    "### 2.1.3 Viusalización del modelo B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4110e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a1_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a1_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a1_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4e0ed",
   "metadata": {},
   "source": [
    "### 2.1.4 Entrenamiento de modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c13d7-c177-4fba-937b-d88d8aa130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a1_r\", model_name=\"model_b_a1_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_r, print_msg=\"Training model B-A1 Raw\", data_loader=train_sub_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a1_b\", model_name=\"model_b_a1_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_b, print_msg=\"Training model B-A1 Bilateral\", data_loader=train_sub_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a1_c\", model_name=\"model_b_a1_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_c, print_msg=\"Training model B-A1 Canny\", data_loader=train_sub_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fbe6b",
   "metadata": {},
   "source": [
    "### 2.1.5 Evaluación de modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf8998-f27f-411e-8ba2-8b8be7de2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A1 Raw\")\n",
    "evaluate_model(model=model_b_a1_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A1 Bilateral\")\n",
    "evaluate_model(model=model_b_a1_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A1 Canny\")\n",
    "evaluate_model(model=model_b_a1_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3b560",
   "metadata": {},
   "source": [
    "### 2.1.6 Matriz de confusión de los modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A1 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a1_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A1 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a1_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A1 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a1_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674a673",
   "metadata": {},
   "source": [
    "## 2.2 Arquitectura B-A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66994d38",
   "metadata": {},
   "source": [
    "### 2.2.1 Definición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=(256 * 16 * 16), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN2.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.inception1(x)                  # [32, 32, 64, 64] -incep-> [32, 128, 64, 64]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 64, 64] -conv-> [32, 128, 32, 32] -ReLu-> same -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv3(x)))    # [32, 128, 32, 32] -conv-> [32, 256, 32, 32] -ReLu-> same -pooling-> [32, 256, 16, 16]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 256 * 16 * 16)           # [32, 256, 16, 16] -view-> [32, 256 * 16 * 16]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 256 * 16 * 16] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc7598",
   "metadata": {},
   "source": [
    "#### Justificación de la arquitectura B-A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124a60d",
   "metadata": {},
   "source": [
    "La arquitectura de la CNN propuesta se enfoca en combinar una estructura eficiente y de bajo costo computacional, mientras aprovecha técnicas avanzadas como el bloque Inception para capturar características de manera exhaustiva y en múltiples escalas. A continuación, se detallan y justifican los componentes clave de la red:\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "Este modelo emplea tres capas convolucionales que aumentan progresivamente el número de filtros (32, 128, 256), todas con un tamaño de kernel de 3x3 y padding. Estas capas tienen la finalidad de:\n",
    "\n",
    "- **Extracción de características detalladas**: El uso de capas adicionales con mayor número de canales permite captar patrones más complejos y específicos a medida que la información progresa a través de la red.\n",
    "- **Tamaño de kernel 3x3**: Este tamaño es eficiente tanto en precisión como en costo computacional, lo que permite detectar patrones de forma local en las imágenes, preservando al mismo tiempo detalles importantes.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "La integración de un bloque Inception después de la primera capa convolucional permite una representación rica de las características de la imagen al procesarla en diferentes escalas:\n",
    "\n",
    "- **Convoluciones con múltiples tamaños de kernel (1x1, 3x3, 5x5)**: Permiten detectar características en diferentes escalas de manera simultánea, enriqueciendo la capacidad de la red para comprender variaciones espaciales en la imagen.\n",
    "- **Reducción de dimensionalidad con convoluciones de 1x1**: El uso de este tamaño de kernel reduce el número de canales, ayudando a optimizar el procesamiento sin perder información relevante.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "El modelo emplea Max Pooling después de cada convolución, con un tamaño de kernel de 2x2, para reducir las dimensiones espaciales de la imagen:\n",
    "\n",
    "- **Reducción de complejidad**: Al disminuir las dimensiones espaciales, Max Pooling reduce el número de parámetros en las capas completamente conectadas (fully connected), limitando la posibilidad de sobreajuste.\n",
    "- **Retención de información clave**: Esta capa selecciona el valor máximo en cada región, ayudando a conservar las características más destacadas y a descartar datos redundantes.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "El modelo utiliza dos capas completamente conectadas (`fc1` y `fc2`) para resumir la información y realizar la clasificación final:\n",
    "\n",
    "- **Compresión de la representación**: La salida de la última capa convolucional se convierte en un vector mediante `view`, y luego pasa por la capa `fc1` de 64 neuronas para compactar la información relevante en un formato adecuado para la etapa de clasificación.\n",
    "- **Clasificación en 3 clases**: La capa `fc2` genera una salida de 3 neuronas, que corresponde a las tres clases de este problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "Para mejorar la generalización del modelo y reducir el sobreajuste, se aplica dropout en la capa `fc1` antes de la salida final:\n",
    "\n",
    "- **Mejora de la robustez**: Dropout aleatoriamente desactiva neuronas durante el entrenamiento, evitando la dependencia en ciertas conexiones y ayudando a la red a aprender representaciones más robustas.\n",
    "\n",
    "##### 6. **Transformación de `x` a través de la red**\n",
    "El tensor de entrada `x` sigue una serie de transformaciones a medida que pasa por las capas de la red. A continuación se detallan estas transformaciones:\n",
    "\n",
    "**x** = `[32, 1, 128, 128]`\n",
    "1. `[32, 1, 128, 128]` **Conv1** → `[32, 32, 128, 128]` → **ReLU** → `[32, 32, 128, 128]` → **MaxPool** → `[32, 32, 64, 64]`\n",
    "2. `[32, 32, 64, 64]` → **Inception1** → `[32, 128, 64, 64]`\n",
    "3. `[32, 128, 64, 64]` → **Conv2** → `[32, 128, 64, 64]` → **ReLU** → `[32, 128, 64, 64]` → **MaxPool** → `[32, 128, 32, 32]`\n",
    "4. `[32, 128, 32, 32]` → **Conv3** → `[32, 256, 32, 32]` → **ReLU** → `[32, 256, 32, 32]` → **MaxPool** → `[32, 256, 16, 16]`\n",
    "5. `[32, 256, 16, 16]` → **View** → `[32, 256 * 16 * 16]` → **Fully Connected (fc1)** → `[32, 64]`\n",
    "6. `[32, 64]` → **Dropout** → **Fully Connected (fc2)** → `[32, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2e1f",
   "metadata": {},
   "source": [
    "### 2.2.2 Creación de los modelos B-A2 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a2_r = CNN2()  # Modelo para datos crudos\n",
    "model_b_a2_b = CNN2()  # Modelo para datos con filtro bilateral\n",
    "model_b_a2_c = CNN2()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9d74e",
   "metadata": {},
   "source": [
    "### 2.2.3 Viusalización del modelo B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a2_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a2_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a2_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d1939",
   "metadata": {},
   "source": [
    "### 2.2.4 Entrenamiento de modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a2_r\", model_name=\"model_b_a2_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_r, print_msg=\"Training model B-A2 Raw\", data_loader=train_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a2_b\", model_name=\"model_b_a2_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_b, print_msg=\"Training model B-A2 Bilateral\", data_loader=train_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a2_c\", model_name=\"model_b_a2_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_c, print_msg=\"Training model B-A2 Canny\", data_loader=train_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4cc02",
   "metadata": {},
   "source": [
    "### 2.2.5 Evaluación de modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A2 Raw\")\n",
    "evaluate_model(model=model_b_a2_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A2 Bilateral\")\n",
    "evaluate_model(model=model_b_a2_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A2 Canny\")\n",
    "evaluate_model(model=model_b_a2_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7aaab",
   "metadata": {},
   "source": [
    "### 2.2.6 Matriz de confusión de los modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4366ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A2 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a2_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A2 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a2_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A2 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a2_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e538a",
   "metadata": {},
   "source": [
    "## 2.3 Arquitectura 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56696436",
   "metadata": {},
   "source": [
    "### 2.3.1 Definición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=(512 * 8 * 8), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN3.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.pool(self.inception1(x))       # [32, 32, 64, 64] -incep-> [32, 128, 64, 64] -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 32, 32] -conv-> [32, 256, 32, 32] -ReLu-> same -pooling-> [32, 256, 16, 16]\n",
    "        x = self.pool(F.relu(self.conv3(x)))    # [32, 256, 16, 16] -conv-> [32, 512, 16, 16] -ReLu-> same -pooling-> [32, 512, 8, 8]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 512 * 8 * 8)             # [32, 512, 8, 8] -view-> [32, 512 * 8 * 8]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 512 * 8 * 8] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ba81b",
   "metadata": {},
   "source": [
    "#### Justificación de la arquitectura B-A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ef6f7",
   "metadata": {},
   "source": [
    "La arquitectura CNN3 propuesta combina eficiencia computacional con técnicas avanzadas como el bloque Inception para capturar características a múltiples escalas, optimizando el procesamiento de imágenes para la clasificación en tres clases. A continuación, se describe y justifica cada componente clave de la red.\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "   - **Capas progresivas de convolución**: Esta arquitectura emplea tres capas convolucionales principales, aumentando el número de filtros a través de las capas (32, 256, 512). Esta progresión permite a la red captar patrones cada vez más complejos a medida que la imagen pasa por cada etapa de convolución.\n",
    "   - **Kernel de 3x3 con padding**: El tamaño del kernel es 3x3, una opción eficiente para detectar patrones locales en imágenes, manteniendo el balance entre precisión y costo computacional.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "   - **Representación en múltiples escalas**: La inclusión del bloque Inception después de la primera capa convolucional permite la captura de características en diversas escalas mediante la combinación de diferentes tamaños de kernel (1x1, 3x3, 5x5). Esto enriquece la capacidad de la red para comprender variaciones espaciales complejas en la imagen.\n",
    "   - **Convolución 1x1 para reducción de dimensionalidad**: El uso de convoluciones de 1x1 reduce el número de canales en los bloques de mayor tamaño, ayudando a optimizar el procesamiento sin sacrificar información clave.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "   - **Reducción espacial y de complejidad**: Se aplica Max Pooling después de cada convolución y después del bloque Inception, con un kernel de 2x2. Esto disminuye las dimensiones espaciales, lo que reduce la complejidad del modelo y el riesgo de sobreajuste.\n",
    "   - **Conservación de características importantes**: Al seleccionar el valor máximo en cada región de la imagen, Max Pooling preserva las características más destacadas y descarta datos redundantes.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "   - **Compresión de la representación**: La salida de la última capa convolucional se convierte en un vector y se pasa por la capa `fc1` de 64 neuronas, lo que permite una representación compacta adecuada para la clasificación final.\n",
    "   - **Clasificación en 3 clases**: La capa `fc2` genera una salida de 3 neuronas, que corresponde a las tres clases de este problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "   - **Regularización del modelo**: Dropout se aplica en la capa `fc1` durante el entrenamiento, desactivando aleatoriamente algunas neuronas. Esto mejora la robustez del modelo al evitar la dependencia en conexiones específicas, ayudando a la red a generalizar mejor.\n",
    "\n",
    "##### 6. **Transformación de `x` a través de la red**\n",
    "El tensor de entrada `x` sigue una serie de transformaciones a medida que pasa por las capas de la red. A continuación se detalla este flujo:\n",
    "\n",
    "**x** = `[32, 1, 128, 128]`  \n",
    "1. `[32, 1, 128, 128]` → **Conv1** → `[32, 32, 128, 128]` → **ReLU** → `[32, 32, 128, 128]` → **MaxPool** → `[32, 32, 64, 64]`  \n",
    "2. `[32, 32, 64, 64]` → **Inception1** → `[32, 128, 64, 64]` → **MaxPool** → `[32, 128, 32, 32]`  \n",
    "3. `[32, 128, 32, 32]` → **Conv2** → `[32, 256, 32, 32]` → **ReLU** → `[32, 256, 32, 32]` → **MaxPool** → `[32, 256, 16, 16]`  \n",
    "4. `[32, 256, 16, 16]` → **Conv3** → `[32, 512, 16, 16]` → **ReLU** → `[32, 512, 16, 16]` → **MaxPool** → `[32, 512, 8, 8]`  \n",
    "5. `[32, 512, 8, 8]` → **View** → `[32, 512 * 8 * 8]` → **Fully Connected (fc1)** → `[32, 64]`  \n",
    "6. `[32, 64]` → **Dropout** → **Fully Connected (fc2)** → `[32, 3]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4621d0",
   "metadata": {},
   "source": [
    "### 2.3.2 Creación de los modelos B-A3 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a3_r = CNN3()  # Modelo para datos crudos\n",
    "model_b_a3_b = CNN3()  # Modelo para datos con filtro bilateral\n",
    "model_b_a3_c = CNN3()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1492fc3",
   "metadata": {},
   "source": [
    "### 2.3.3 Viusalización del modelo B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a3_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a3_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a3_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96561b0",
   "metadata": {},
   "source": [
    "### 2.3.4 Entrenamiento de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a3_r\", model_name=\"model_b_a3_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_r, print_msg=\"Training model B-A3 Raw\", data_loader=train_sub_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a3_b\", model_name=\"model_b_a3_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_b, print_msg=\"Training model B-A3 Bilateral\", data_loader=train_sub_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a3_c\", model_name=\"model_b_a3_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_c, print_msg=\"Training model B-A3 Canny\", data_loader=train_sub_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0240f",
   "metadata": {},
   "source": [
    "### 2.3.5 Evaluación de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42681717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A3 Raw\")\n",
    "evaluate_model(model=model_b_a3_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A3 Bilateral\")\n",
    "evaluate_model(model=model_b_a3_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A3 Canny\")\n",
    "evaluate_model(model=model_b_a3_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb7b2d",
   "metadata": {},
   "source": [
    "### 2.3.6 Matriz de confusión de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A3 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a3_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A3 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a3_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A3 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a3_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dac8b6",
   "metadata": {},
   "source": [
    "# 3. Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322c447",
   "metadata": {},
   "source": [
    "## 1. **CNN1**: \n",
    "   - Esta red comienza con una capa convolucional que tiene 32 filtros, seguida de un bloque Inception. El bloque Inception toma la entrada de 32 canales y la expande a 128 canales combinando varias convoluciones (1x1, 3x3, 5x5) y una operación de pooling.\n",
    "   - Luego, tiene una segunda capa convolucional de 128 canales. \n",
    "   - Después del procesamiento convolucional, los datos se transforman mediante capas totalmente conectadas (fully connected), comenzando con una capa de 64 neuronas y finalizando con la capa de salida que tiene 3 neuronas, lo que indica que se está trabajando con un problema de clasificación de tres clases.\n",
    "   - Las dimensiones de las imágenes se reducen progresivamente por medio de operaciones de \"pooling\" y \"view\", hasta que los datos son transformados para pasar por las capas fully connected.\n",
    "\n",
    "### **Pros**:\n",
    "  - Arquitectura simple y menos profunda, lo que la hace más rápida de entrenar.\n",
    "  - Menor riesgo de sobreajuste, ya que tiene menos parámetros que las otras redes.\n",
    "  - Menor consumo de recursos computacionales (memoria y tiempo de entrenamiento).\n",
    "### **Contras**:\n",
    "  - Menor capacidad para aprender características complejas, lo que puede limitar su rendimiento en problemas más difíciles.\n",
    "  - Menos filtros y capas pueden hacerla menos precisa en tareas de clasificación más complejas.\n",
    "\n",
    "\n",
    "## 2. **CNN2**:\n",
    "   - En este modelo, la arquitectura es similar a CNN1, pero incluye una tercera capa convolucional adicional. Después de la segunda capa convolucional con 128 filtros, CNN2 aplica una tercera convolución que incrementa los canales a 256.\n",
    "   - La primera parte de la red sigue siendo casi idéntica: una capa convolucional, un bloque Inception y una segunda convolución. Sin embargo, la tercera convolución aumenta la capacidad del modelo para extraer características más complejas.\n",
    "   - Al final, la red transforma los datos con una capa fully connected de 64 neuronas, seguida de la capa de salida de 3 neuronas.\n",
    "\n",
    "### **Pros**:\n",
    "  - Mayor capacidad que CNN1 gracias a la adición de una tercera capa convolucional con más filtros.\n",
    "  - Puede aprender características más complejas, lo que mejora su desempeño en problemas más difíciles.\n",
    "  - Aumenta la complejidad sin ser excesivamente pesada en términos de cómputo.\n",
    "### **Contras**:\n",
    "  - Mayor riesgo de sobreajuste comparado con CNN1, ya que tiene más parámetros.\n",
    "  - Consume más recursos (memoria y tiempo de entrenamiento) que CNN1.\n",
    "\n",
    "\n",
    "## 3. **CNN3**:\n",
    "   - Este modelo es el más profundo de los tres. Tras la capa convolucional inicial y el bloque Inception, tiene una segunda convolución que aumenta los canales a 256 (similar a CNN2), pero además, añade una tercera capa convolucional que incrementa los canales a 512.\n",
    "   - El tamaño de las características se reduce progresivamente con cada operación de \"pooling\", y después de la tercera convolución, las dimensiones son mucho menores (8x8), lo que implica que CNN3 está preparada para aprender características más abstractas.\n",
    "   - Finalmente, la red tiene las capas fully connected y de salida, similares a CNN2, pero con una mayor cantidad de información procesada en las etapas previas.\n",
    "\n",
    "### **Pros**:\n",
    "  - La arquitectura más profunda y con más filtros la hace más poderosa para capturar características abstractas y complejas.\n",
    "  - Potencialmente mejor desempeño en tareas con patrones complicados o grandes cantidades de datos.\n",
    "### **Contras**:\n",
    "  - Alto riesgo de sobreajuste si no se cuenta con suficientes datos o técnicas de regularización.\n",
    "  - Mayor demanda de recursos computacionales, con tiempos de entrenamiento más largos y mayor uso de memoria.\n",
    "  - Puede ser innecesaria para tareas simples, donde una red menos profunda sería más eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b9a28",
   "metadata": {},
   "source": [
    "## 3.1 Selección de la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679751a2",
   "metadata": {},
   "source": [
    "La arquitectura CNN2 fue seleccionada como la mejor opción, ya que además de ser la que mejores resultados obtuvo (accuracy), también combina una capacidad intermedia para aprender características complejas con un ciste computacional manejable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_IC6200_AI_P2_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
