{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7450d71-b3fb-4acb-8715-73ce35f9b9fc",
   "metadata": {},
   "source": [
    "<!-- PROJECT LOGO -->\n",
    "<br />\n",
    "<div align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"https://res.cloudinary.com/dek4evg4t/image/upload/v1729273000/Group_4.png\" alt=\"Logo\" width=\"30%\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "### üñπ Descripci√≥n:\n",
    "Este Proyecto tiene como objetivo aplicar redes neuronales convolucionales (CNN) para realizar una clasificaci√≥n multiclase de im√°genes mediante aprendizaje supervisado. Utilizando el [Covid-19 Image Dataset de Kaggle](https://www.kaggle.com/datasets/pranavraikokte/covid19-image-dataset), que contiene im√°genes de rayos X clasificadas en tres categor√≠as (Covid-19, Normal, Neumon√≠a), en este proyecto se desarrollar√°n clasificadores capaces de diagnosticar enfermedades pulmonares. El proyecto tambi√©n explora el uso de PyTorch para el desarrollo de modelos de Machine Learning y herramientas de monitoreo, como Weights and Biases, para el seguimiento en tiempo real del proceso de entrenamiento.\n",
    "\n",
    "### ‚úçÔ∏è Autores:\n",
    "* Angelo Ortiz Vega - [@angelortizv](https://github.com/angelortizv)\n",
    "* Alejandro Campos Abarca - [@MajinLoop](https://github.com/MajinLoop)\n",
    "\n",
    "### üìÖ Fecha:\n",
    "20 de octubre de 2024\n",
    "\n",
    "### üìù Notas:\n",
    "Este es el segundo proyecto del curso IC6200 - Inteligencia Artificial. En este notebook, titulado \"Covid-19 Classification\", se profundiza en t√©cnicas de data augmentation, preprocesamiento de im√°genes con filtros, y fine-tuning de modelos CNN como VGG16 para mejorar la capacidad de generalizaci√≥n de las redes neuronales convolucionales.\n",
    "\n",
    "### Otras notas:\n",
    "Asegurarse de contar con Python y las siguientes bibliotecas instaladas: torch, torchvision, cv2, numpy, matplotlib, Pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59076502",
   "metadata": {},
   "source": [
    "# 1. Configuraci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb193973-d2be-44d9-9aa2-6b77c6853646",
   "metadata": {},
   "source": [
    "## 1.1 Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecfc9c-01ea-47af-a3cb-907e031dafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from torchviz import make_dot\n",
    "os.environ[\"PATH\"] += r\";C:\\Program Files\\Graphviz\\bin\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0244b6e",
   "metadata": {},
   "source": [
    "## 1.2 Verificar PyThorch y CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c823e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if(torch.cuda.is_available()):\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a7e24-bb26-4dc5-9d85-16c332c33c37",
   "metadata": {},
   "source": [
    "## 1.3 Definici√≥n de Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a36b3-41b3-49df-9d28-cc8db2174e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_VALUE = 153\n",
    "IMAGE_RESIZE = 128\n",
    "IMAGE_NORMALIZE = 0.5\n",
    "\n",
    "HYPERPARAMETERS = \\\n",
    "{\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"dropout_rate\": 0.5,\n",
    "}\n",
    "\n",
    "# Paths\n",
    "TRAIN_DATA_PATH = 'data/Covid19-dataset/train'\n",
    "TEST_DATA_PATH = 'data/Covid19-dataset/test'\n",
    "VISUALIZATION_DIR = 'visualization'\n",
    "MODEL_B_GRAPHS_DIR = 'Model_B_graphs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7afea1",
   "metadata": {},
   "source": [
    "## 1.4 Configuraci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d093e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch settings\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "\n",
    "# Configuraci√≥n de Weights & Biases\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27921004",
   "metadata": {},
   "source": [
    "## 1.5 Definici√≥n de filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gausian_blur(img, order=21, sigma=21):\n",
    "    np_img = np.array(img)\n",
    "    filtered_img = cv2.GaussianBlur(np_img, (order,order), sigma)\n",
    "    return Image.fromarray(filtered_img)\n",
    "\n",
    "def bilateral_filter(img):\n",
    "    \"\"\"\n",
    "    d: Diameter of each pixel neighborhood.\n",
    "    \n",
    "    sigmaColor: Value of \\sigma in the color space. The greater the value, the colors farther to each other will start to get mixed.\n",
    "    \n",
    "    sigmaSpace: Value of \\sigma in the coordinate space. The greater its value, the more further pixels will mix together, given that\n",
    "    their colors lie within the sigmaColor range.    \n",
    "    \"\"\"\n",
    "    np_img = np.array(img)\n",
    "    filtered_img = cv2.bilateralFilter(np_img, 15, 75, 75)\n",
    "    return Image.fromarray(filtered_img) \n",
    "\n",
    "def canny_edge_filter(img, min_val=40, max_val=85):\n",
    "    img = np.array(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # PIL (RGB) -> OpenCV (BGR)\n",
    "    img = cv2.Canny(img, min_val, max_val)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV (BGR) -> PIL (RGB)\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff29366-f8e8-4e64-93fd-3f7471a65a5e",
   "metadata": {},
   "source": [
    "## 1.6 Creaci√≥n de los dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d6376",
   "metadata": {},
   "source": [
    "### 1.6.1 Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_r = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_r_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotaci√≥n aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotaci√≥n\n",
    "        \n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_b = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Lambda(lambda img: bilateral_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_b_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotaci√≥n aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotaci√≥n\n",
    "\n",
    "        transforms.Lambda(lambda img: bilateral_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_c = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "        transforms.Lambda(lambda img: canny_edge_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")\n",
    "transform_c_augmented = transforms.Compose \\\n",
    "(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "\n",
    "        transforms.RandomHorizontalFlip(),                          # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(30),                              # Rotaci√≥n aleatoria de hasta 30 grados\n",
    "        transforms.RandomAffine(degrees=30, translate=(0.1, 0.1)),  # Desplazamiento aleatorio con rotaci√≥n\n",
    "\n",
    "        transforms.Lambda(lambda img: canny_edge_filter(img)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((IMAGE_NORMALIZE,), (IMAGE_NORMALIZE,))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519de310",
   "metadata": {},
   "source": [
    "### 1.6.2 Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482fd7a-4983-42d6-84d0-c950afcee3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_data_loader(dataset, fraction_of_data=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    subset_size = int(np.floor(fraction_of_data * dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    sub_dataset = Subset(dataset, subset_indices)\n",
    "    # print(f'Subset size: {len(sub_dataset)}')\n",
    "    return DataLoader(dataset=sub_dataset, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Train data\n",
    "# Raw\n",
    "train_dataset_r = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_r)\n",
    "train_dataset_r_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_r_augmented)\n",
    "combined_train_dataset_r = ConcatDataset([train_dataset_r, train_dataset_r_augmented])\n",
    "train_loader_r = DataLoader(dataset=combined_train_dataset_r, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_r = get_sub_data_loader(dataset=combined_train_dataset_r)\n",
    "\n",
    "# Bilateral\n",
    "train_dataset_b = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_b_augmented)\n",
    "train_dataset_b_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_b_augmented)\n",
    "combined_train_dataset_b = ConcatDataset([train_dataset_b, train_dataset_b_augmented])\n",
    "train_loader_b = DataLoader(dataset=combined_train_dataset_b, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_b = get_sub_data_loader(dataset=combined_train_dataset_b)\n",
    "\n",
    "# Canny\n",
    "train_dataset_c = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_c_augmented)\n",
    "train_dataset_c_augmented = datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=transform_c_augmented)\n",
    "combined_train_dataset_c = ConcatDataset([train_dataset_c, train_dataset_c_augmented])\n",
    "train_loader_c = DataLoader(dataset=combined_train_dataset_c, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "train_sub_loader_c = get_sub_data_loader(dataset=combined_train_dataset_c)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Test data\n",
    "# Raw\n",
    "test_dataset_r = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_r_augmented)\n",
    "test_loader_r = DataLoader(dataset=test_dataset_r, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "# Bilateral\n",
    "test_dataset_b = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_b_augmented)\n",
    "test_loader_b = DataLoader(dataset=test_dataset_b, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "# Canny\n",
    "test_dataset_c = datasets.ImageFolder(root=TEST_DATA_PATH, transform=transform_c_augmented)\n",
    "test_loader_c = DataLoader(dataset=test_dataset_c, batch_size=HYPERPARAMETERS[\"batch_size\"], shuffle=True)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(f'Tama√±o del conjunto de entrenamiento: {len(train_loader_r.dataset)}')\n",
    "print(f'Tama√±o del conjunto de testing: {len(test_loader_r.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78657128-b115-4cd4-bf2b-1f1e1a70980c",
   "metadata": {},
   "source": [
    "## 1.7 Visualizaci√≥n de los filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde176b-bbef-4d03-a52a-a052f9cbe24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_five_images(data_loader, dataset):\n",
    "    \n",
    "    try:\n",
    "        images, labels = next(iter(data_loader))\n",
    "    \n",
    "    except StopIteration:\n",
    "        print(\"El DataLoader no tiene m√°s datos.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ocurri√≥ un error: {e}\")\n",
    "    \n",
    "    images = images.numpy().transpose((0, 2, 3, 1))\n",
    "    images = np.clip(images, 0, 1)\n",
    "\n",
    "    _, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for ax, img, label in zip(axes, images[:5], labels[:5]):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Clase: {dataset.classes[label.item()]}')\n",
    "    plt.show()\n",
    "\n",
    "show_first_five_images(train_loader_r, train_dataset_r)\n",
    "show_first_five_images(train_loader_b, train_dataset_b)\n",
    "show_first_five_images(train_loader_c, train_dataset_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30d06b",
   "metadata": {},
   "source": [
    "Los colores se ven inusuales porque `plt` asume que la imagen est√° en formato RGB, pero en realidad solo tiene un canal de color. Esto se debe a que el modelo est√° dise√±ado para aceptar entradas con solo un canal de color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f7547",
   "metadata": {},
   "source": [
    "## 1.8 Funci√≥n de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, model_name):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    initial_wandb_learning_rate = wandb.config.learning_rate # Guardar el learning rate inicial\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "\n",
    "        accumulated_epoch_loss = 0\n",
    "        epoch_correct_predictions = 0\n",
    "        epoch_total_samples = 0\n",
    "\n",
    "        # For batch\n",
    "        for images, labels in train_data_loader:\n",
    "\n",
    "            # Limpiar gradientes: Se restablecen los gradientes acumulados en el optimizador a cero para evitar que se sumen a los gradientes del lote actual.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Se pasan las im√°genes a trav√©s del modelo para obtener las predicciones (model_predictions).\n",
    "            model_predictions = model(images)\n",
    "\n",
    "            # C√°lculo de la p√©rdida: Se calcula la p√©rdida utilizando la funci√≥n de p√©rdida definida anteriormente,\n",
    "            # comparando las predicciones (model_predictions) con las etiquetas verdaderas (labels).\n",
    "            batch_loss = criterion(model_predictions, labels)\n",
    "            \n",
    "            # Sumar la p√©rdida de este lote a la p√©rdida total de la √©poca\n",
    "            accumulated_epoch_loss += batch_loss.item()\n",
    "\n",
    "            _, predicted = torch.max(model_predictions, 1)  # Obtener las predicciones con el mayor valor (clase m√°s probable)\n",
    "            batch_correct_predictions = (predicted == labels).sum().item()    # Contar cu√°ntas predicciones son correctas\n",
    "            total_samples = labels.size(0)                          # Total de elementos en el batch\n",
    "\n",
    "            epoch_correct_predictions += batch_correct_predictions\n",
    "            epoch_total_samples += total_samples\n",
    "\n",
    "            # Backward pass: Se realiza la propagaci√≥n hacia atr√°s para calcular los gradientes de la p√©rdida con respecto a los par√°metros del modelo.\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Actualizar par√°metros: Se actualizan los par√°metros del modelo utilizando los gradientes calculados en la etapa anterior.\n",
    "            optimizer.step()\n",
    "\n",
    "        # C√°lculo de la accuracy promedio de la √©poca\n",
    "        epoch_loss = accumulated_epoch_loss / len(train_data_loader)\n",
    "        epoch_accuracy = 100 * epoch_correct_predictions / epoch_total_samples\n",
    "\n",
    "        # Imprimir p√©rdida promedio y accuracy por √©poca, as√≠ como el LR\n",
    "        current_lr = optimizer.param_groups[0]['lr']    # Obtener el learning rate actual\n",
    "        print(f'Epoch [{epoch + 1}/{wandb.config.epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # WandB\n",
    "        wandb.config.update({\"learning_rate\": current_lr})\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": epoch_loss, \"accuracy\": epoch_accuracy, \"model\": model_name}, commit=True)\n",
    "\n",
    "        scheduler.step(epoch_loss)\n",
    "    \n",
    "    # Restaurar el valor inicial de learning rate despu√©s del entrenamiento\n",
    "    wandb.config.update({\"learning_rate\": initial_wandb_learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6d846",
   "metadata": {},
   "source": [
    "- **mode**='min': El scheduler se activar√° si la p√©rdida se encuentra en su m√≠nimo.\n",
    "- **factor**=0.5: Reducir√° el learning rate a la mitad.\n",
    "- **patience**=2: Esperar√° 2 √©pocas antes de reducir el learning rate si no hay mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfa191",
   "metadata": {},
   "source": [
    "## 1.9 Funciones de evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b315aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images, labels in test_data_loader:\n",
    "\n",
    "            model_predictions = model(images)\n",
    "            _, predicted = torch.max(model_predictions.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_wandb_project(\n",
    "        project_name: str,\n",
    "        model_name: str,\n",
    "        hyperparameters: dict,\n",
    "        model,\n",
    "        print_msg: str,\n",
    "        data_loader: DataLoader,\n",
    "        wandb_watch_log = \"all\"):\n",
    "    \n",
    "    wandb.init(project=project_name, name=model_name)\n",
    "    wandb.config.update(hyperparameters)\n",
    "    wandb.watch(model, log=wandb_watch_log)\n",
    "    print(print_msg)\n",
    "    train(model=model, train_data_loader=data_loader, model_name=model)\n",
    "    print()\n",
    "    wandb.unwatch(model)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255d354",
   "metadata": {},
   "source": [
    "# 2. Modelo B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727ecae",
   "metadata": {},
   "source": [
    "## 2.1 Arquitectura B-A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f7da2",
   "metadata": {},
   "source": [
    "### 2.1.1 Definici√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e67a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=(128 * 16 * 16), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN1.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.pool(self.inception1(x))       # [32, 32, 64, 64] -incep-> [32, 128, 64, 64] -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 32, 32] -conv-> [32, 128, 32, 32] -ReLu-> same -pooling-> [32, 128, 16, 16]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 128 * 16 * 16)           # [32, 128, 16, 16] -view-> [32, 128*16*16]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 128*16*16] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01b71e",
   "metadata": {},
   "source": [
    "#### Justificaci√≥n de la arquitectura B-A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958726f",
   "metadata": {},
   "source": [
    "La arquitectura propuesta para esta CNN est√° dise√±ada de manera que logre un equilibrio entre simplicidad y capacidad de extracci√≥n de caracter√≠sticas en im√°genes. A continuaci√≥n, justifico las decisiones tomadas en la estructura del modelo:\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "Se han utilizado tres capas convolucionales con un n√∫mero creciente de filtros (32, 128) y un tama√±o de kernel de 3x3, junto con un bloque Inception. Esta configuraci√≥n permite:\n",
    "- **Extracci√≥n progresiva de caracter√≠sticas**: Al aumentar el n√∫mero de canales, la red puede capturar detalles de baja a alta complejidad a medida que las im√°genes pasan por las capas.\n",
    "- **Receptive field**: El tama√±o de 3x3 es un est√°ndar eficaz que equilibra precisi√≥n y costo computacional, permitiendo que la red capture patrones locales en las im√°genes.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "La incorporaci√≥n de un bloque Inception permite que la red capture una variedad m√°s amplia de caracter√≠sticas a diferentes escalas. Las ventajas incluyen:\n",
    "- **M√∫ltiples tama√±os de convoluci√≥n**: El bloque utiliza convoluciones con diferentes tama√±os de kernel (1x1, 3x3, 5x5) en paralelo, permitiendo que la red aprenda a detectar caracter√≠sticas en diversas escalas.\n",
    "- **Reducci√≥n de dimensionalidad**: Las convoluciones de 1x1 ayudan a reducir el n√∫mero de canales, optimizando la carga computacional.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "El uso de `MaxPool2D` con kernel 2x2 reduce las dimensiones espaciales de las im√°genes (de 128x128 a 16x16 tras 3 pasos), permitiendo:\n",
    "- **Reducci√≥n de la dimensionalidad**: Lo cual es clave para disminuir el n√∫mero de par√°metros en las capas completamente conectadas (fully connected), evitando el sobreajuste.\n",
    "- **Retenci√≥n de informaci√≥n importante**: El max pooling retiene los valores m√°ximos en cada regi√≥n, preservando caracter√≠sticas clave mientras descarta informaci√≥n redundante.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "Se ha elegido un n√∫mero moderado de neuronas en las capas `fc1` (64) y `fc2` (3), con las siguientes razones:\n",
    "- **Compactaci√≥n de las caracter√≠sticas**: La salida de la √∫ltima capa convolucional se transforma en un vector de caracter√≠sticas mediante `view`, y las capas fully connected resumen esta informaci√≥n en un formato √∫til para la clasificaci√≥n.\n",
    "- **Reducci√≥n final a 3 clases**: La capa `fc2` genera una salida de 3 neuronas, correspondiente a las 3 categor√≠as del problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "Se introduce dropout en las √∫ltimas iteraciones con el fin de:\n",
    "- **Evitar el sobreajuste**: Dropout es una t√©cnica eficaz para mejorar la generalizaci√≥n del modelo, ya que previene la dependencia excesiva en ciertos patrones durante el entrenamiento.\n",
    "\n",
    "##### 6. **Resultados experimentales**\n",
    "El desempe√±o del modelo mejora notablemente a lo largo de las iteraciones al ajustar los hiperpar√°metros (learning rate, epochs) y al incluir dropout. Esto valida que la arquitectura es capaz de aprender representaciones √∫tiles cuando se le da suficiente tiempo de entrenamiento y se aplican t√©cnicas de regularizaci√≥n adecuadas.\n",
    "\n",
    "##### 7. **Etapas**\n",
    "Las **etapas** de esta red CNN se pueden dividir en tres bloques principales:\n",
    "\n",
    "###### 1. **Etapa de extracci√≥n de caracter√≠sticas**\n",
    "   - **Capas**: `conv1`, `inception1`, `conv2`, y `pool`\n",
    "   - **Descripci√≥n**: Esta etapa utiliza capas convolucionales y de pooling para extraer caracter√≠sticas de la imagen de entrada. La activaci√≥n `ReLU` a√±ade no linealidad y el `MaxPool2D` reduce la dimensionalidad de los mapas de caracter√≠sticas.\n",
    "     - Conv1: Extrae caracter√≠sticas iniciales de la imagen (bordes, texturas).\n",
    "     - Inception1: Captura caracter√≠sticas en m√∫ltiples escalas.\n",
    "     - Conv2: Extrae a√∫n m√°s caracter√≠sticas complejas.\n",
    "     - MaxPool: Reduce la dimensi√≥n espacial despu√©s de cada convoluci√≥n.\n",
    "\n",
    "###### 2. **Etapa de aplanamiento (Flattening)**\n",
    "   - **Capas**: `x.view(-1, 128 * 16 * 16)`\n",
    "   - **Descripci√≥n**: Aplana la salida de la √∫ltima capa convolucional en un vector para poder pasarla a las capas totalmente conectadas.\n",
    "\n",
    "###### 3. **Etapa de clasificaci√≥n**\n",
    "   - **Capas**: `fc1`, `dropout`, `fc2`\n",
    "   - **Descripci√≥n**: Las capas completamente conectadas (`fc1` y `fc2`) se encargan de la clasificaci√≥n final. `fc1` reduce la dimensionalidad a 64 neuronas, y despu√©s se aplica un `dropout` para prevenir el sobreajuste. Finalmente, `fc2` produce la salida de 3 clases (asumiendo una clasificaci√≥n de 3 categor√≠as).\n",
    "\n",
    "Estas tres etapas forman la estructura completa de la CNN.\n",
    "\n",
    "##### Transformaci√≥n de `x` a trav√©s de la red\n",
    "El tensor de entrada `x` tiene la forma `[N, C, H, W]`, donde:\n",
    "- **N** = Batch size (n√∫mero de muestras).\n",
    "- **C** = N√∫mero de canales.\n",
    "- **H** = Altura de la entrada.\n",
    "- **W** = Ancho de la entrada.\n",
    "\n",
    "- **Input shape**: `[32, 1, 128, 128]`\n",
    "1. `[32, 1, 128, 128]` ‚Üí **Conv1** ‚Üí `[32, 32, 128, 128]` ‚Üí **ReLU** ‚Üí `[32, 32, 128, 128]` ‚Üí **Pooling** ‚Üí `[32, 32, 64, 64]`\n",
    "2. `[32, 32, 64, 64]` ‚Üí **Inception1** ‚Üí `[32, 128, 64, 64]` ‚Üí **Pooling** ‚Üí `[32, 128, 32, 32]`\n",
    "3. `[32, 128, 32, 32]` ‚Üí **Conv2** ‚Üí `[32, 128, 32, 32]` ‚Üí **ReLU** ‚Üí `[32, 128, 32, 32]` ‚Üí **Pooling** ‚Üí `[32, 128, 16, 16]`\n",
    "4. `[32, 128, 16, 16]` ‚Üí **View** ‚Üí `[32, 128 * 16 * 16]`\n",
    "5. `[32, 128 * 16 * 16]` ‚Üí **Fully Connected (fc1)** ‚Üí `[32, 64]`\n",
    "6. `[32, 64]` ‚Üí **Fully Connected (fc2)** ‚Üí `[32, 3]`\n",
    "\n",
    "###### `x.view()` function:\n",
    "La funci√≥n `view()` en PyTorch se utiliza para cambiar la forma de un tensor sin alterar sus datos subyacentes.\n",
    "Transforma el tensor de entrada de una forma 4D `[N, C, H, W]` a una forma 2D `[N, F]`, donde:\n",
    "- **F** = N√∫mero de caracter√≠sticas (features).\n",
    "El n√∫mero de caracter√≠sticas se calcula como:\n",
    "\\[\n",
    "F = C \\times H \\times W\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad7db3",
   "metadata": {},
   "source": [
    "#### Explicaci√≥n de las capas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae7096",
   "metadata": {},
   "source": [
    "##### Capas de Pooling\n",
    "Una capa de pooling reduce la dimensionalidad del mapa de caracter√≠sticas generado por la capa de convoluci√≥n. Funciona tomando un subconjunto de valores (por ejemplo, el valor m√°ximo o promedio) dentro de una ventana deslizante, lo que disminuye el tama√±o de la representaci√≥n. Esto ayuda a preservar las caracter√≠sticas m√°s importantes mientras se descartan detalles menos relevantes.\n",
    "La reducci√≥n en la dimensionalidad tambi√©n mejora la eficiencia computacional y ayuda a prevenir el sobreajuste. En resumen, el pooling simplifica la representaci√≥n y permite que el modelo se enfoque en las caracter√≠sticas m√°s significativas.\n",
    "\n",
    "###### C√°lculo de la Dimensionalidad:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d' = \\frac{d - k}{s} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Par√°metros:\n",
    "- **d -> dimension_in**: Es el tama√±o de la dimension de entrada\n",
    "- **k -> kernel_size**: Es el tama√±o del kernel (matriz de orden $k^2$).\n",
    "- **s -> stride**: El n√∫mero de p√≠xeles que el filtro se desplaza sobre la imagen de entrada, con un valor de 1 significando que se mueve un p√≠xel a la vez.\n",
    "- **d' -> dimension_out**: Es el tama√±o de la dimension de salida.\n",
    "\n",
    "##### Capas de convoluci√≥n\n",
    "Una capa de convoluci√≥n extrae caracter√≠sticas de la imagen de entrada mediante filtros que se deslizan sobre la misma. \n",
    "Aplica una operaci√≥n matem√°tica llamada convoluci√≥n, produciendo un mapa de caracter√≠sticas que resalta patrones como bordes y texturas. \n",
    "Mantiene la estructura espacial de la imagen, permitiendo que el modelo aprenda relaciones espaciales. \n",
    "Generalmente, se le aplica una funci√≥n de activaci√≥n (como ReLU) para introducir no linealidades.\n",
    "\n",
    "###### C√°lculo de la Dimensionalidad:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d' = \\frac{d + 2p - k}{s} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Par√°metros:\n",
    "- **k -> kernel_size**: Es el orden de una matriz cuadrada.\n",
    "- **s -> stride**: El n√∫mero de p√≠xeles que el filtro se desplaza sobre la imagen de entrada, con un valor de 1 significando que se mueve un p√≠xel a la vez.\n",
    "- **p -> padding**: El n√∫mero de p√≠xeles a√±adidos alrededor de la imagen de entrada para mantener las dimensiones de salida. El padding ayuda a prevenir que la imagen se reduzca demasiado despu√©s de la convoluci√≥n, permitiendo una mejor retenci√≥n de informaci√≥n en los bordes.\n",
    "\n",
    "\n",
    "##### Capas Fully Connected\n",
    "Una capa totalmente conectada (fully connected layer) conecta cada neurona de la capa anterior a cada neurona de la capa actual. Se utiliza para combinar las caracter√≠sticas extra√≠das por las capas previas y realizar la clasificaci√≥n o regresi√≥n. \n",
    "Cada neurona en esta capa aplica una transformaci√≥n lineal a las entradas, seguida de una funci√≥n de activaci√≥n, lo que permite al modelo aprender patrones complejos. Las capas totalmente conectadas suelen encontrarse al final de una red neuronal, despu√©s de las capas de convoluci√≥n y pooling.\n",
    "En resumen, estas capas integran la informaci√≥n y producen la salida final del modelo.\n",
    "\n",
    "\n",
    "##### Capa Dropout\n",
    "La capa `Dropout` es una t√©cnica de regularizaci√≥n utilizada para prevenir el sobreajuste durante el entrenamiento de redes neuronales. En cada paso de entrenamiento, desactiva aleatoriamente un porcentaje de neuronas (definido por `DROPOUT_RATE`), lo que obliga a la red a no depender demasiado de una sola neurona. Esto mejora la generalizaci√≥n del modelo, ayudando a que aprenda patrones m√°s robustos. En esta CNN, se aplica despu√©s de la primera capa completamente conectada para mejorar el rendimiento en tareas de clasificaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0751-fc07-4e8d-8490-65c04f45857e",
   "metadata": {},
   "source": [
    "### 2.1.2 Creaci√≥n de los modelos B-A1 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef183fe-154c-4f86-a5d3-5fe294692448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a1_r = CNN1()  # Modelo para datos crudos\n",
    "model_b_a1_b = CNN1()  # Modelo para datos con filtro bilateral\n",
    "model_b_a1_c = CNN1()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86faecb3",
   "metadata": {},
   "source": [
    "### 2.1.3 Viusalizaci√≥n del modelo B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4110e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a1_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a1_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a1_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4e0ed",
   "metadata": {},
   "source": [
    "### 2.1.4 Entrenamiento de modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c13d7-c177-4fba-937b-d88d8aa130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a1_r\", model_name=\"model_b_a1_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_r, print_msg=\"Training model B-A1 Raw\", data_loader=train_sub_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a1_b\", model_name=\"model_b_a1_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_b, print_msg=\"Training model B-A1 Bilateral\", data_loader=train_sub_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a1_c\", model_name=\"model_b_a1_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a1_c, print_msg=\"Training model B-A1 Canny\", data_loader=train_sub_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fbe6b",
   "metadata": {},
   "source": [
    "### 2.1.5 Evaluaci√≥n de modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf8998-f27f-411e-8ba2-8b8be7de2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A1 Raw\")\n",
    "evaluate_model(model=model_b_a1_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A1 Bilateral\")\n",
    "evaluate_model(model=model_b_a1_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A1 Canny\")\n",
    "evaluate_model(model=model_b_a1_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3b560",
   "metadata": {},
   "source": [
    "### 2.1.6 Matriz de confusi√≥n de los modelos B-A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A1 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a1_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A1 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a1_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A1 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a1_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674a673",
   "metadata": {},
   "source": [
    "## 2.2 Arquitectura B-A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66994d38",
   "metadata": {},
   "source": [
    "### 2.2.1 Definici√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=(256 * 16 * 16), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN2.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.inception1(x)                  # [32, 32, 64, 64] -incep-> [32, 128, 64, 64]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 64, 64] -conv-> [32, 128, 32, 32] -ReLu-> same -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv3(x)))    # [32, 128, 32, 32] -conv-> [32, 256, 32, 32] -ReLu-> same -pooling-> [32, 256, 16, 16]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 256 * 16 * 16)           # [32, 256, 16, 16] -view-> [32, 256 * 16 * 16]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 256 * 16 * 16] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc7598",
   "metadata": {},
   "source": [
    "#### Justificaci√≥n de la arquitectura B-A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124a60d",
   "metadata": {},
   "source": [
    "La arquitectura de la CNN propuesta se enfoca en combinar una estructura eficiente y de bajo costo computacional, mientras aprovecha t√©cnicas avanzadas como el bloque Inception para capturar caracter√≠sticas de manera exhaustiva y en m√∫ltiples escalas. A continuaci√≥n, se detallan y justifican los componentes clave de la red:\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "Este modelo emplea tres capas convolucionales que aumentan progresivamente el n√∫mero de filtros (32, 128, 256), todas con un tama√±o de kernel de 3x3 y padding. Estas capas tienen la finalidad de:\n",
    "\n",
    "- **Extracci√≥n de caracter√≠sticas detalladas**: El uso de capas adicionales con mayor n√∫mero de canales permite captar patrones m√°s complejos y espec√≠ficos a medida que la informaci√≥n progresa a trav√©s de la red.\n",
    "- **Tama√±o de kernel 3x3**: Este tama√±o es eficiente tanto en precisi√≥n como en costo computacional, lo que permite detectar patrones de forma local en las im√°genes, preservando al mismo tiempo detalles importantes.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "La integraci√≥n de un bloque Inception despu√©s de la primera capa convolucional permite una representaci√≥n rica de las caracter√≠sticas de la imagen al procesarla en diferentes escalas:\n",
    "\n",
    "- **Convoluciones con m√∫ltiples tama√±os de kernel (1x1, 3x3, 5x5)**: Permiten detectar caracter√≠sticas en diferentes escalas de manera simult√°nea, enriqueciendo la capacidad de la red para comprender variaciones espaciales en la imagen.\n",
    "- **Reducci√≥n de dimensionalidad con convoluciones de 1x1**: El uso de este tama√±o de kernel reduce el n√∫mero de canales, ayudando a optimizar el procesamiento sin perder informaci√≥n relevante.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "El modelo emplea Max Pooling despu√©s de cada convoluci√≥n, con un tama√±o de kernel de 2x2, para reducir las dimensiones espaciales de la imagen:\n",
    "\n",
    "- **Reducci√≥n de complejidad**: Al disminuir las dimensiones espaciales, Max Pooling reduce el n√∫mero de par√°metros en las capas completamente conectadas (fully connected), limitando la posibilidad de sobreajuste.\n",
    "- **Retenci√≥n de informaci√≥n clave**: Esta capa selecciona el valor m√°ximo en cada regi√≥n, ayudando a conservar las caracter√≠sticas m√°s destacadas y a descartar datos redundantes.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "El modelo utiliza dos capas completamente conectadas (`fc1` y `fc2`) para resumir la informaci√≥n y realizar la clasificaci√≥n final:\n",
    "\n",
    "- **Compresi√≥n de la representaci√≥n**: La salida de la √∫ltima capa convolucional se convierte en un vector mediante `view`, y luego pasa por la capa `fc1` de 64 neuronas para compactar la informaci√≥n relevante en un formato adecuado para la etapa de clasificaci√≥n.\n",
    "- **Clasificaci√≥n en 3 clases**: La capa `fc2` genera una salida de 3 neuronas, que corresponde a las tres clases de este problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "Para mejorar la generalizaci√≥n del modelo y reducir el sobreajuste, se aplica dropout en la capa `fc1` antes de la salida final:\n",
    "\n",
    "- **Mejora de la robustez**: Dropout aleatoriamente desactiva neuronas durante el entrenamiento, evitando la dependencia en ciertas conexiones y ayudando a la red a aprender representaciones m√°s robustas.\n",
    "\n",
    "##### 6. **Transformaci√≥n de `x` a trav√©s de la red**\n",
    "El tensor de entrada `x` sigue una serie de transformaciones a medida que pasa por las capas de la red. A continuaci√≥n se detallan estas transformaciones:\n",
    "\n",
    "**x** = `[32, 1, 128, 128]`\n",
    "1. `[32, 1, 128, 128]` **Conv1** ‚Üí `[32, 32, 128, 128]` ‚Üí **ReLU** ‚Üí `[32, 32, 128, 128]` ‚Üí **MaxPool** ‚Üí `[32, 32, 64, 64]`\n",
    "2. `[32, 32, 64, 64]` ‚Üí **Inception1** ‚Üí `[32, 128, 64, 64]`\n",
    "3. `[32, 128, 64, 64]` ‚Üí **Conv2** ‚Üí `[32, 128, 64, 64]` ‚Üí **ReLU** ‚Üí `[32, 128, 64, 64]` ‚Üí **MaxPool** ‚Üí `[32, 128, 32, 32]`\n",
    "4. `[32, 128, 32, 32]` ‚Üí **Conv3** ‚Üí `[32, 256, 32, 32]` ‚Üí **ReLU** ‚Üí `[32, 256, 32, 32]` ‚Üí **MaxPool** ‚Üí `[32, 256, 16, 16]`\n",
    "5. `[32, 256, 16, 16]` ‚Üí **View** ‚Üí `[32, 256 * 16 * 16]` ‚Üí **Fully Connected (fc1)** ‚Üí `[32, 64]`\n",
    "6. `[32, 64]` ‚Üí **Dropout** ‚Üí **Fully Connected (fc2)** ‚Üí `[32, 3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f2e1f",
   "metadata": {},
   "source": [
    "### 2.2.2 Creaci√≥n de los modelos B-A2 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a2_r = CNN2()  # Modelo para datos crudos\n",
    "model_b_a2_b = CNN2()  # Modelo para datos con filtro bilateral\n",
    "model_b_a2_c = CNN2()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9d74e",
   "metadata": {},
   "source": [
    "### 2.2.3 Viusalizaci√≥n del modelo B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a2_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a2_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a2_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d1939",
   "metadata": {},
   "source": [
    "### 2.2.4 Entrenamiento de modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a2_r\", model_name=\"model_b_a2_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_r, print_msg=\"Training model B-A2 Raw\", data_loader=train_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a2_b\", model_name=\"model_b_a2_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_b, print_msg=\"Training model B-A2 Bilateral\", data_loader=train_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a2_c\", model_name=\"model_b_a2_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a2_c, print_msg=\"Training model B-A2 Canny\", data_loader=train_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4cc02",
   "metadata": {},
   "source": [
    "### 2.2.5 Evaluaci√≥n de modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A2 Raw\")\n",
    "evaluate_model(model=model_b_a2_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A2 Bilateral\")\n",
    "evaluate_model(model=model_b_a2_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A2 Canny\")\n",
    "evaluate_model(model=model_b_a2_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7aaab",
   "metadata": {},
   "source": [
    "### 2.2.6 Matriz de confusi√≥n de los modelos B-A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4366ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A2 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a2_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A2 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a2_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A2 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a2_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e538a",
   "metadata": {},
   "source": [
    "## 2.3 Arquitectura 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56696436",
   "metadata": {},
   "source": [
    "### 2.3.1 Definici√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.inception1 = self.InceptionBlock(in_channels=32) # -> 128\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=(512 * 8 * 8), out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=3)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(HYPERPARAMETERS[\"dropout_rate\"])\n",
    "\n",
    "    class InceptionBlock(nn.Module):\n",
    "        def __init__(self, in_channels):\n",
    "            super(CNN3.InceptionBlock, self).__init__()\n",
    "            self.branch1x1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch3x3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch5x5_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "            self.branch5x5_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch_pool = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1 = self.branch1x1(x)\n",
    "\n",
    "            branch2 = self.branch3x3_1(x)\n",
    "            branch2 = self.branch3x3_2(branch2)\n",
    "\n",
    "            branch3 = self.branch5x5_1(x)\n",
    "            branch3 = self.branch5x5_2(branch3)\n",
    "\n",
    "            branch4 = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch4 = self.branch_pool(branch4)\n",
    "\n",
    "            outputs = [branch1, branch2, branch3, branch4]\n",
    "            return torch.cat(outputs, 1) # (32 + 32 + 32 + 32 = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Etapa 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # [32, 1, 128, 128] -conv-> [32, 32, 128, 128] -ReLu-> same -pooling-> [32, 32, 64, 64]\n",
    "        x = self.pool(self.inception1(x))       # [32, 32, 64, 64] -incep-> [32, 128, 64, 64] -pooling-> [32, 128, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))    # [32, 128, 32, 32] -conv-> [32, 256, 32, 32] -ReLu-> same -pooling-> [32, 256, 16, 16]\n",
    "        x = self.pool(F.relu(self.conv3(x)))    # [32, 256, 16, 16] -conv-> [32, 512, 16, 16] -ReLu-> same -pooling-> [32, 512, 8, 8]\n",
    "        # Etapa 2\n",
    "        x = x.view(-1, 512 * 8 * 8)             # [32, 512, 8, 8] -view-> [32, 512 * 8 * 8]\n",
    "        # Etapa 3\n",
    "        x = F.relu(self.fc1(x))                 # [32, 512 * 8 * 8] -fc-> [32, 64]\n",
    "        x = self.dropout(x)                     # Aplicar Dropout\n",
    "        x = self.fc2(x)                         # [32, 64] -fc-> [32, 3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ba81b",
   "metadata": {},
   "source": [
    "#### Justificaci√≥n de la arquitectura B-A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ef6f7",
   "metadata": {},
   "source": [
    "La arquitectura CNN3 propuesta combina eficiencia computacional con t√©cnicas avanzadas como el bloque Inception para capturar caracter√≠sticas a m√∫ltiples escalas, optimizando el procesamiento de im√°genes para la clasificaci√≥n en tres clases. A continuaci√≥n, se describe y justifica cada componente clave de la red.\n",
    "\n",
    "##### 1. **Capas convolucionales (Conv2D)**\n",
    "   - **Capas progresivas de convoluci√≥n**: Esta arquitectura emplea tres capas convolucionales principales, aumentando el n√∫mero de filtros a trav√©s de las capas (32, 256, 512). Esta progresi√≥n permite a la red captar patrones cada vez m√°s complejos a medida que la imagen pasa por cada etapa de convoluci√≥n.\n",
    "   - **Kernel de 3x3 con padding**: El tama√±o del kernel es 3x3, una opci√≥n eficiente para detectar patrones locales en im√°genes, manteniendo el balance entre precisi√≥n y costo computacional.\n",
    "\n",
    "##### 2. **Bloque Inception**\n",
    "   - **Representaci√≥n en m√∫ltiples escalas**: La inclusi√≥n del bloque Inception despu√©s de la primera capa convolucional permite la captura de caracter√≠sticas en diversas escalas mediante la combinaci√≥n de diferentes tama√±os de kernel (1x1, 3x3, 5x5). Esto enriquece la capacidad de la red para comprender variaciones espaciales complejas en la imagen.\n",
    "   - **Convoluci√≥n 1x1 para reducci√≥n de dimensionalidad**: El uso de convoluciones de 1x1 reduce el n√∫mero de canales en los bloques de mayor tama√±o, ayudando a optimizar el procesamiento sin sacrificar informaci√≥n clave.\n",
    "\n",
    "##### 3. **Max Pooling**\n",
    "   - **Reducci√≥n espacial y de complejidad**: Se aplica Max Pooling despu√©s de cada convoluci√≥n y despu√©s del bloque Inception, con un kernel de 2x2. Esto disminuye las dimensiones espaciales, lo que reduce la complejidad del modelo y el riesgo de sobreajuste.\n",
    "   - **Conservaci√≥n de caracter√≠sticas importantes**: Al seleccionar el valor m√°ximo en cada regi√≥n de la imagen, Max Pooling preserva las caracter√≠sticas m√°s destacadas y descarta datos redundantes.\n",
    "\n",
    "##### 4. **Capas completamente conectadas (Fully Connected)**\n",
    "   - **Compresi√≥n de la representaci√≥n**: La salida de la √∫ltima capa convolucional se convierte en un vector y se pasa por la capa `fc1` de 64 neuronas, lo que permite una representaci√≥n compacta adecuada para la clasificaci√≥n final.\n",
    "   - **Clasificaci√≥n en 3 clases**: La capa `fc2` genera una salida de 3 neuronas, que corresponde a las tres clases de este problema.\n",
    "\n",
    "##### 5. **Dropout**\n",
    "   - **Regularizaci√≥n del modelo**: Dropout se aplica en la capa `fc1` durante el entrenamiento, desactivando aleatoriamente algunas neuronas. Esto mejora la robustez del modelo al evitar la dependencia en conexiones espec√≠ficas, ayudando a la red a generalizar mejor.\n",
    "\n",
    "##### 6. **Transformaci√≥n de `x` a trav√©s de la red**\n",
    "El tensor de entrada `x` sigue una serie de transformaciones a medida que pasa por las capas de la red. A continuaci√≥n se detalla este flujo:\n",
    "\n",
    "**x** = `[32, 1, 128, 128]`  \n",
    "1. `[32, 1, 128, 128]` ‚Üí **Conv1** ‚Üí `[32, 32, 128, 128]` ‚Üí **ReLU** ‚Üí `[32, 32, 128, 128]` ‚Üí **MaxPool** ‚Üí `[32, 32, 64, 64]`  \n",
    "2. `[32, 32, 64, 64]` ‚Üí **Inception1** ‚Üí `[32, 128, 64, 64]` ‚Üí **MaxPool** ‚Üí `[32, 128, 32, 32]`  \n",
    "3. `[32, 128, 32, 32]` ‚Üí **Conv2** ‚Üí `[32, 256, 32, 32]` ‚Üí **ReLU** ‚Üí `[32, 256, 32, 32]` ‚Üí **MaxPool** ‚Üí `[32, 256, 16, 16]`  \n",
    "4. `[32, 256, 16, 16]` ‚Üí **Conv3** ‚Üí `[32, 512, 16, 16]` ‚Üí **ReLU** ‚Üí `[32, 512, 16, 16]` ‚Üí **MaxPool** ‚Üí `[32, 512, 8, 8]`  \n",
    "5. `[32, 512, 8, 8]` ‚Üí **View** ‚Üí `[32, 512 * 8 * 8]` ‚Üí **Fully Connected (fc1)** ‚Üí `[32, 64]`  \n",
    "6. `[32, 64]` ‚Üí **Dropout** ‚Üí **Fully Connected (fc2)** ‚Üí `[32, 3]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4621d0",
   "metadata": {},
   "source": [
    "### 2.3.2 Creaci√≥n de los modelos B-A3 (Raw, Bilateral y Canny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b_a3_r = CNN3()  # Modelo para datos crudos\n",
    "model_b_a3_b = CNN3()  # Modelo para datos con filtro bilateral\n",
    "model_b_a3_c = CNN3()  # Modelo para datos con filtro Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1492fc3",
   "metadata": {},
   "source": [
    "### 2.3.3 Viusalizaci√≥n del modelo B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 128, 128)  # Tensor de ejemplo\n",
    "y = model_b_a3_r(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model_b_a3_r.named_parameters())))\n",
    "dot.attr(dpi='300')\n",
    "file_name = 'model_b_a3_graph'\n",
    "directory = os.path.join(VISUALIZATION_DIR, MODEL_B_GRAPHS_DIR)\n",
    "dot.render(filename=file_name, directory=directory, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96561b0",
   "metadata": {},
   "source": [
    "### 2.3.4 Entrenamiento de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_wandb_project(project_name=\"project_b_a3_r\", model_name=\"model_b_a3_r\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_r, print_msg=\"Training model B-A3 Raw\", data_loader=train_sub_loader_r)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a3_b\", model_name=\"model_b_a3_b\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_b, print_msg=\"Training model B-A3 Bilateral\", data_loader=train_sub_loader_b)\n",
    "\n",
    "make_wandb_project(project_name=\"project_b_a3_c\", model_name=\"model_b_a3_c\", hyperparameters=HYPERPARAMETERS,\n",
    "                   model=model_b_a3_c, print_msg=\"Training model B-A3 Canny\", data_loader=train_sub_loader_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0240f",
   "metadata": {},
   "source": [
    "### 2.3.5 Evaluaci√≥n de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42681717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model B-A3 Raw\")\n",
    "evaluate_model(model=model_b_a3_r, test_data_loader=test_loader_r)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A3 Bilateral\")\n",
    "evaluate_model(model=model_b_a3_b, test_data_loader=test_loader_b)\n",
    "print()\n",
    "\n",
    "print(\"Evaluating model B-A3 Canny\")\n",
    "evaluate_model(model=model_b_a3_c, test_data_loader=test_loader_c)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb7b2d",
   "metadata": {},
   "source": [
    "### 2.3.6 Matriz de confusi√≥n de los modelos B-A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando modelo B-A3 Raw\")\n",
    "y_true_r, y_pred_r = get_predictions(model=model_b_a3_r, data_loader=test_loader_r)\n",
    "plot_confusion_matrix(y_true_r, y_pred_r, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A3 Bilateral\")\n",
    "y_true_b, y_pred_b = get_predictions(model=model_b_a3_b, data_loader=test_loader_b)\n",
    "plot_confusion_matrix(y_true_b, y_pred_b, class_names=['Clase 0', 'Clase 1', 'Clase 2'])\n",
    "\n",
    "print(\"Evaluando modelo B-A3 Canny\")\n",
    "y_true_c, y_pred_c = get_predictions(model=model_b_a3_c, data_loader=test_loader_c)\n",
    "plot_confusion_matrix(y_true_c, y_pred_c, class_names=['Clase 0', 'Clase 1', 'Clase 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dac8b6",
   "metadata": {},
   "source": [
    "# 3. An√°lisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322c447",
   "metadata": {},
   "source": [
    "## 1. **CNN1**: \n",
    "   - Esta red comienza con una capa convolucional que tiene 32 filtros, seguida de un bloque Inception. El bloque Inception toma la entrada de 32 canales y la expande a 128 canales combinando varias convoluciones (1x1, 3x3, 5x5) y una operaci√≥n de pooling.\n",
    "   - Luego, tiene una segunda capa convolucional de 128 canales. \n",
    "   - Despu√©s del procesamiento convolucional, los datos se transforman mediante capas totalmente conectadas (fully connected), comenzando con una capa de 64 neuronas y finalizando con la capa de salida que tiene 3 neuronas, lo que indica que se est√° trabajando con un problema de clasificaci√≥n de tres clases.\n",
    "   - Las dimensiones de las im√°genes se reducen progresivamente por medio de operaciones de \"pooling\" y \"view\", hasta que los datos son transformados para pasar por las capas fully connected.\n",
    "\n",
    "### **Pros**:\n",
    "  - Arquitectura simple y menos profunda, lo que la hace m√°s r√°pida de entrenar.\n",
    "  - Menor riesgo de sobreajuste, ya que tiene menos par√°metros que las otras redes.\n",
    "  - Menor consumo de recursos computacionales (memoria y tiempo de entrenamiento).\n",
    "### **Contras**:\n",
    "  - Menor capacidad para aprender caracter√≠sticas complejas, lo que puede limitar su rendimiento en problemas m√°s dif√≠ciles.\n",
    "  - Menos filtros y capas pueden hacerla menos precisa en tareas de clasificaci√≥n m√°s complejas.\n",
    "\n",
    "\n",
    "## 2. **CNN2**:\n",
    "   - En este modelo, la arquitectura es similar a CNN1, pero incluye una tercera capa convolucional adicional. Despu√©s de la segunda capa convolucional con 128 filtros, CNN2 aplica una tercera convoluci√≥n que incrementa los canales a 256.\n",
    "   - La primera parte de la red sigue siendo casi id√©ntica: una capa convolucional, un bloque Inception y una segunda convoluci√≥n. Sin embargo, la tercera convoluci√≥n aumenta la capacidad del modelo para extraer caracter√≠sticas m√°s complejas.\n",
    "   - Al final, la red transforma los datos con una capa fully connected de 64 neuronas, seguida de la capa de salida de 3 neuronas.\n",
    "\n",
    "### **Pros**:\n",
    "  - Mayor capacidad que CNN1 gracias a la adici√≥n de una tercera capa convolucional con m√°s filtros.\n",
    "  - Puede aprender caracter√≠sticas m√°s complejas, lo que mejora su desempe√±o en problemas m√°s dif√≠ciles.\n",
    "  - Aumenta la complejidad sin ser excesivamente pesada en t√©rminos de c√≥mputo.\n",
    "### **Contras**:\n",
    "  - Mayor riesgo de sobreajuste comparado con CNN1, ya que tiene m√°s par√°metros.\n",
    "  - Consume m√°s recursos (memoria y tiempo de entrenamiento) que CNN1.\n",
    "\n",
    "\n",
    "## 3. **CNN3**:\n",
    "   - Este modelo es el m√°s profundo de los tres. Tras la capa convolucional inicial y el bloque Inception, tiene una segunda convoluci√≥n que aumenta los canales a 256 (similar a CNN2), pero adem√°s, a√±ade una tercera capa convolucional que incrementa los canales a 512.\n",
    "   - El tama√±o de las caracter√≠sticas se reduce progresivamente con cada operaci√≥n de \"pooling\", y despu√©s de la tercera convoluci√≥n, las dimensiones son mucho menores (8x8), lo que implica que CNN3 est√° preparada para aprender caracter√≠sticas m√°s abstractas.\n",
    "   - Finalmente, la red tiene las capas fully connected y de salida, similares a CNN2, pero con una mayor cantidad de informaci√≥n procesada en las etapas previas.\n",
    "\n",
    "### **Pros**:\n",
    "  - La arquitectura m√°s profunda y con m√°s filtros la hace m√°s poderosa para capturar caracter√≠sticas abstractas y complejas.\n",
    "  - Potencialmente mejor desempe√±o en tareas con patrones complicados o grandes cantidades de datos.\n",
    "### **Contras**:\n",
    "  - Alto riesgo de sobreajuste si no se cuenta con suficientes datos o t√©cnicas de regularizaci√≥n.\n",
    "  - Mayor demanda de recursos computacionales, con tiempos de entrenamiento m√°s largos y mayor uso de memoria.\n",
    "  - Puede ser innecesaria para tareas simples, donde una red menos profunda ser√≠a m√°s eficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b9a28",
   "metadata": {},
   "source": [
    "## 3.1 Selecci√≥n de la arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679751a2",
   "metadata": {},
   "source": [
    "La arquitectura CNN2 fue seleccionada como la mejor opci√≥n, ya que adem√°s de ser la que mejores resultados obtuvo (accuracy), tambi√©n combina una capacidad intermedia para aprender caracter√≠sticas complejas con un ciste computacional manejable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_IC6200_AI_P2_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
