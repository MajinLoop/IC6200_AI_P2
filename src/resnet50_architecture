digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1777652514736 [label="
 (1, 3)" fillcolor=darkolivegreen1]
	1777670337776 [label=AddmmBackward0]
	1777670337728 -> 1777670337776
	1777670307680 [label="fc.bias
 (3)" fillcolor=lightblue]
	1777670307680 -> 1777670337728
	1777670337728 [label=AccumulateGrad]
	1777652710176 -> 1777670337776
	1777652710176 [label=ViewBackward0]
	1777652698656 -> 1777652710176
	1777652698656 [label=MeanBackward1]
	1777652698224 -> 1777652698656
	1777652698224 [label=ReluBackward0]
	1777652697120 -> 1777652698224
	1777652697120 [label=AddBackward0]
	1777652697024 -> 1777652697120
	1777652697024 [label=CudnnBatchNormBackward0]
	1777652696256 -> 1777652697024
	1777652696256 [label=ConvolutionBackward0]
	1777652706912 -> 1777652696256
	1777652706912 [label=ReluBackward0]
	1777652706576 -> 1777652706912
	1777652706576 [label=CudnnBatchNormBackward0]
	1777652708352 -> 1777652706576
	1777652708352 [label=ConvolutionBackward0]
	1777652704128 -> 1777652708352
	1777652704128 [label=ReluBackward0]
	1777652702832 -> 1777652704128
	1777652702832 [label=CudnnBatchNormBackward0]
	1777652704896 -> 1777652702832
	1777652704896 [label=ConvolutionBackward0]
	1777652696592 -> 1777652704896
	1777652696592 [label=ReluBackward0]
	1777652702112 -> 1777652696592
	1777652702112 [label=AddBackward0]
	1777652708448 -> 1777652702112
	1777652708448 [label=CudnnBatchNormBackward0]
	1777652702256 -> 1777652708448
	1777652702256 [label=ConvolutionBackward0]
	1777652702784 -> 1777652702256
	1777652702784 [label=ReluBackward0]
	1777652702352 -> 1777652702784
	1777652702352 [label=CudnnBatchNormBackward0]
	1777652702400 -> 1777652702352
	1777652702400 [label=ConvolutionBackward0]
	1777652701632 -> 1777652702400
	1777652701632 [label=ReluBackward0]
	1777652704704 -> 1777652701632
	1777652704704 [label=CudnnBatchNormBackward0]
	1777652703792 -> 1777652704704
	1777652703792 [label=ConvolutionBackward0]
	1777652708688 -> 1777652703792
	1777652708688 [label=ReluBackward0]
	1777652697984 -> 1777652708688
	1777652697984 [label=AddBackward0]
	1777652696688 -> 1777652697984
	1777652696688 [label=CudnnBatchNormBackward0]
	1777652699328 -> 1777652696688
	1777652699328 [label=ConvolutionBackward0]
	1777652695920 -> 1777652699328
	1777652695920 [label=ReluBackward0]
	1777652709504 -> 1777652695920
	1777652709504 [label=CudnnBatchNormBackward0]
	1777652697792 -> 1777652709504
	1777652697792 [label=ConvolutionBackward0]
	1777652706000 -> 1777652697792
	1777652706000 [label=ReluBackward0]
	1777652706288 -> 1777652706000
	1777652706288 [label=CudnnBatchNormBackward0]
	1777652706336 -> 1777652706288
	1777652706336 [label=ConvolutionBackward0]
	1772110804400 -> 1777652706336
	1772110804400 [label=ReluBackward0]
	1772110801088 -> 1772110804400
	1772110801088 [label=AddBackward0]
	1772110804208 -> 1772110801088
	1772110804208 [label=CudnnBatchNormBackward0]
	1772110802816 -> 1772110804208
	1772110802816 [label=ConvolutionBackward0]
	1772110802528 -> 1772110802816
	1772110802528 [label=ReluBackward0]
	1772110806416 -> 1772110802528
	1772110806416 [label=CudnnBatchNormBackward0]
	1772110805936 -> 1772110806416
	1772110805936 [label=ConvolutionBackward0]
	1772110802672 -> 1772110805936
	1772110802672 [label=ReluBackward0]
	1772110801040 -> 1772110802672
	1772110801040 [label=CudnnBatchNormBackward0]
	1772110800992 -> 1772110801040
	1772110800992 [label=ConvolutionBackward0]
	1772110794464 -> 1772110800992
	1772110794464 [label=ReluBackward0]
	1772110799408 -> 1772110794464
	1772110799408 [label=AddBackward0]
	1777653094416 -> 1772110799408
	1777653094416 [label=CudnnBatchNormBackward0]
	1777653094848 -> 1777653094416
	1777653094848 [label=ConvolutionBackward0]
	1777653099696 -> 1777653094848
	1777653099696 [label=ReluBackward0]
	1777653100992 -> 1777653099696
	1777653100992 [label=CudnnBatchNormBackward0]
	1777653094608 -> 1777653100992
	1777653094608 [label=ConvolutionBackward0]
	1773228555904 -> 1777653094608
	1773228555904 [label=ReluBackward0]
	1772111321824 -> 1773228555904
	1772111321824 [label=CudnnBatchNormBackward0]
	1772111330848 -> 1772111321824
	1772111330848 [label=ConvolutionBackward0]
	1777653092736 -> 1772111330848
	1777653092736 [label=ReluBackward0]
	1772111328640 -> 1777653092736
	1772111328640 [label=AddBackward0]
	1772111329888 -> 1772111328640
	1772111329888 [label=CudnnBatchNormBackward0]
	1772111329984 -> 1772111329888
	1772111329984 [label=ConvolutionBackward0]
	1772111329312 -> 1772111329984
	1772111329312 [label=ReluBackward0]
	1772111330224 -> 1772111329312
	1772111330224 [label=CudnnBatchNormBackward0]
	1772111330704 -> 1772111330224
	1772111330704 [label=ConvolutionBackward0]
	1772111329600 -> 1772111330704
	1772111329600 [label=ReluBackward0]
	1772111330656 -> 1772111329600
	1772111330656 [label=CudnnBatchNormBackward0]
	1772111330752 -> 1772111330656
	1772111330752 [label=ConvolutionBackward0]
	1772111330032 -> 1772111330752
	1772111330032 [label=ReluBackward0]
	1772111329648 -> 1772111330032
	1772111329648 [label=AddBackward0]
	1772111315200 -> 1772111329648
	1772111315200 [label=CudnnBatchNormBackward0]
	1772111331088 -> 1772111315200
	1772111331088 [label=ConvolutionBackward0]
	1772111331232 -> 1772111331088
	1772111331232 [label=ReluBackward0]
	1772111330464 -> 1772111331232
	1772111330464 [label=CudnnBatchNormBackward0]
	1772111329264 -> 1772111330464
	1772111329264 [label=ConvolutionBackward0]
	1772111324752 -> 1772111329264
	1772111324752 [label=ReluBackward0]
	1772111329168 -> 1772111324752
	1772111329168 [label=CudnnBatchNormBackward0]
	1772111329456 -> 1772111329168
	1772111329456 [label=ConvolutionBackward0]
	1772111324608 -> 1772111329456
	1772111324608 [label=ReluBackward0]
	1772111315776 -> 1772111324608
	1772111315776 [label=AddBackward0]
	1772111325472 -> 1772111315776
	1772111325472 [label=CudnnBatchNormBackward0]
	1772111315632 -> 1772111325472
	1772111315632 [label=ConvolutionBackward0]
	1772111325856 -> 1772111315632
	1772111325856 [label=ReluBackward0]
	1772111325808 -> 1772111325856
	1772111325808 [label=CudnnBatchNormBackward0]
	1777652841680 -> 1772111325808
	1777652841680 [label=ConvolutionBackward0]
	1777652841104 -> 1777652841680
	1777652841104 [label=ReluBackward0]
	1777652840480 -> 1777652841104
	1777652840480 [label=CudnnBatchNormBackward0]
	1777652840192 -> 1777652840480
	1777652840192 [label=ConvolutionBackward0]
	1772111321056 -> 1777652840192
	1772111321056 [label=ReluBackward0]
	1777652839328 -> 1772111321056
	1777652839328 [label=AddBackward0]
	1777652839040 -> 1777652839328
	1777652839040 [label=CudnnBatchNormBackward0]
	1777652838800 -> 1777652839040
	1777652838800 [label=ConvolutionBackward0]
	1777652838224 -> 1777652838800
	1777652838224 [label=ReluBackward0]
	1777652837600 -> 1777652838224
	1777652837600 [label=CudnnBatchNormBackward0]
	1777652837312 -> 1777652837600
	1777652837312 [label=ConvolutionBackward0]
	1777652836736 -> 1777652837312
	1777652836736 [label=ReluBackward0]
	1777652836496 -> 1777652836736
	1777652836496 [label=CudnnBatchNormBackward0]
	1777652836208 -> 1777652836496
	1777652836208 [label=ConvolutionBackward0]
	1777652835632 -> 1777652836208
	1777652835632 [label=ReluBackward0]
	1777652835008 -> 1777652835632
	1777652835008 [label=AddBackward0]
	1777652834720 -> 1777652835008
	1777652834720 [label=CudnnBatchNormBackward0]
	1777652834480 -> 1777652834720
	1777652834480 [label=ConvolutionBackward0]
	1777652833904 -> 1777652834480
	1777652833904 [label=ReluBackward0]
	1777652833280 -> 1777652833904
	1777652833280 [label=CudnnBatchNormBackward0]
	1777652832992 -> 1777652833280
	1777652832992 [label=ConvolutionBackward0]
	1777652832416 -> 1777652832992
	1777652832416 [label=ReluBackward0]
	1777652832176 -> 1777652832416
	1777652832176 [label=CudnnBatchNormBackward0]
	1777652831888 -> 1777652832176
	1777652831888 [label=ConvolutionBackward0]
	1777652835056 -> 1777652831888
	1777652835056 [label=ReluBackward0]
	1777652831024 -> 1777652835056
	1777652831024 [label=AddBackward0]
	1777652830736 -> 1777652831024
	1777652830736 [label=CudnnBatchNormBackward0]
	1777652830112 -> 1777652830736
	1777652830112 [label=ConvolutionBackward0]
	1777652829536 -> 1777652830112
	1777652829536 [label=ReluBackward0]
	1777652829296 -> 1777652829536
	1777652829296 [label=CudnnBatchNormBackward0]
	1777652829008 -> 1777652829296
	1777652829008 [label=ConvolutionBackward0]
	1777652828432 -> 1777652829008
	1777652828432 [label=ReluBackward0]
	1777652827808 -> 1777652828432
	1777652827808 [label=CudnnBatchNormBackward0]
	1777652827520 -> 1777652827808
	1777652827520 [label=ConvolutionBackward0]
	1777652830688 -> 1777652827520
	1777652830688 [label=ReluBackward0]
	1777652826656 -> 1777652830688
	1777652826656 [label=AddBackward0]
	1777652826368 -> 1777652826656
	1777652826368 [label=CudnnBatchNormBackward0]
	1777652826416 -> 1777652826368
	1777652826416 [label=ConvolutionBackward0]
	1777652809680 -> 1777652826416
	1777652809680 [label=ReluBackward0]
	1777652809056 -> 1777652809680
	1777652809056 [label=CudnnBatchNormBackward0]
	1777652808768 -> 1777652809056
	1777652808768 [label=ConvolutionBackward0]
	1777652808192 -> 1777652808768
	1777652808192 [label=ReluBackward0]
	1777652807952 -> 1777652808192
	1777652807952 [label=CudnnBatchNormBackward0]
	1777652807664 -> 1777652807952
	1777652807664 [label=ConvolutionBackward0]
	1777652826704 -> 1777652807664
	1777652826704 [label=ReluBackward0]
	1777652806800 -> 1777652826704
	1777652806800 [label=AddBackward0]
	1777652806512 -> 1777652806800
	1777652806512 [label=CudnnBatchNormBackward0]
	1777652805888 -> 1777652806512
	1777652805888 [label=ConvolutionBackward0]
	1777652805312 -> 1777652805888
	1777652805312 [label=ReluBackward0]
	1777652805072 -> 1777652805312
	1777652805072 [label=CudnnBatchNormBackward0]
	1777652804784 -> 1777652805072
	1777652804784 [label=ConvolutionBackward0]
	1777652804208 -> 1777652804784
	1777652804208 [label=ReluBackward0]
	1777652803584 -> 1777652804208
	1777652803584 [label=CudnnBatchNormBackward0]
	1777652803296 -> 1777652803584
	1777652803296 [label=ConvolutionBackward0]
	1777652802720 -> 1777652803296
	1777652802720 [label=ReluBackward0]
	1777652802480 -> 1777652802720
	1777652802480 [label=AddBackward0]
	1777652802192 -> 1777652802480
	1777652802192 [label=CudnnBatchNormBackward0]
	1777652801568 -> 1777652802192
	1777652801568 [label=ConvolutionBackward0]
	1777652800992 -> 1777652801568
	1777652800992 [label=ReluBackward0]
	1777652800752 -> 1777652800992
	1777652800752 [label=CudnnBatchNormBackward0]
	1777652800464 -> 1777652800752
	1777652800464 [label=ConvolutionBackward0]
	1777652799888 -> 1777652800464
	1777652799888 [label=ReluBackward0]
	1777652799312 -> 1777652799888
	1777652799312 [label=CudnnBatchNormBackward0]
	1777652798976 -> 1777652799312
	1777652798976 [label=ConvolutionBackward0]
	1777652802144 -> 1777652798976
	1777652802144 [label=ReluBackward0]
	1777652798112 -> 1777652802144
	1777652798112 [label=AddBackward0]
	1777652797824 -> 1777652798112
	1777652797824 [label=CudnnBatchNormBackward0]
	1777652797584 -> 1777652797824
	1777652797584 [label=ConvolutionBackward0]
	1777652797008 -> 1777652797584
	1777652797008 [label=ReluBackward0]
	1777652796384 -> 1777652797008
	1777652796384 [label=CudnnBatchNormBackward0]
	1777652796096 -> 1777652796384
	1777652796096 [label=ConvolutionBackward0]
	1777652795520 -> 1777652796096
	1777652795520 [label=ReluBackward0]
	1777652795280 -> 1777652795520
	1777652795280 [label=CudnnBatchNormBackward0]
	1777652794992 -> 1777652795280
	1777652794992 [label=ConvolutionBackward0]
	1777652798160 -> 1777652794992
	1777652798160 [label=ReluBackward0]
	1777652794128 -> 1777652798160
	1777652794128 [label=AddBackward0]
	1777652793840 -> 1777652794128
	1777652793840 [label=CudnnBatchNormBackward0]
	1777652793552 -> 1777652793840
	1777652793552 [label=ConvolutionBackward0]
	1777652526064 -> 1777652793552
	1777652526064 [label=ReluBackward0]
	1777652528080 -> 1777652526064
	1777652528080 [label=CudnnBatchNormBackward0]
	1777652527648 -> 1777652528080
	1777652527648 [label=ConvolutionBackward0]
	1777652518432 -> 1777652527648
	1777652518432 [label=ReluBackward0]
	1777652518912 -> 1777652518432
	1777652518912 [label=CudnnBatchNormBackward0]
	1777652519056 -> 1777652518912
	1777652519056 [label=ConvolutionBackward0]
	1777652519536 -> 1777652519056
	1777652519536 [label=MaxPool2DWithIndicesBackward0]
	1777652518624 -> 1777652519536
	1777652518624 [label=ReluBackward0]
	1777652519680 -> 1777652518624
	1777652519680 [label=CudnnBatchNormBackward0]
	1777652519824 -> 1777652519680
	1777652519824 [label=ConvolutionBackward0]
	1777652519344 -> 1777652519824
	1777651603696 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1777651603696 -> 1777652519344
	1777652519344 [label=AccumulateGrad]
	1777652519632 -> 1777652519680
	1777670375120 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1777670375120 -> 1777652519632
	1777652519632 [label=AccumulateGrad]
	1777652519248 -> 1777652519680
	1772111376000 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1772111376000 -> 1777652519248
	1777652519248 [label=AccumulateGrad]
	1777652519392 -> 1777652519056
	1772112168736 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1772112168736 -> 1777652519392
	1777652519392 [label=AccumulateGrad]
	1777652518864 -> 1777652518912
	1772112175856 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1772112175856 -> 1777652518864
	1777652518864 [label=AccumulateGrad]
	1777652519200 -> 1777652518912
	1772112176336 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1772112176336 -> 1777652519200
	1777652519200 [label=AccumulateGrad]
	1777652527312 -> 1777652527648
	1772112173696 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1772112173696 -> 1777652527312
	1777652527312 [label=AccumulateGrad]
	1777652526496 -> 1777652528080
	1772112170096 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1772112170096 -> 1777652526496
	1777652526496 [label=AccumulateGrad]
	1777652525008 -> 1777652528080
	1772112175936 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1772112175936 -> 1777652525008
	1777652525008 [label=AccumulateGrad]
	1777652526352 -> 1777652793552
	1772112176416 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1772112176416 -> 1777652526352
	1777652526352 [label=AccumulateGrad]
	1777652793504 -> 1777652793840
	1772112178976 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1772112178976 -> 1777652793504
	1777652793504 [label=AccumulateGrad]
	1777652528224 -> 1777652793840
	1772112179936 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1772112179936 -> 1777652528224
	1777652528224 [label=AccumulateGrad]
	1777652793792 -> 1777652794128
	1777652793792 [label=CudnnBatchNormBackward0]
	1777652525728 -> 1777652793792
	1777652525728 [label=ConvolutionBackward0]
	1777652519536 -> 1777652525728
	1777652518768 -> 1777652525728
	1772111371760 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1772111371760 -> 1777652518768
	1777652518768 [label=AccumulateGrad]
	1777652524912 -> 1777652793792
	1772112181856 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1772112181856 -> 1777652524912
	1777652524912 [label=AccumulateGrad]
	1777652525440 -> 1777652793792
	1772112179296 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1772112179296 -> 1777652525440
	1777652525440 [label=AccumulateGrad]
	1777652794416 -> 1777652794992
	1772112168896 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1772112168896 -> 1777652794416
	1777652794416 [label=AccumulateGrad]
	1777652794944 -> 1777652795280
	1772112169776 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1772112169776 -> 1777652794944
	1777652794944 [label=AccumulateGrad]
	1777652795568 -> 1777652795280
	1772112179136 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1772112179136 -> 1777652795568
	1777652795568 [label=AccumulateGrad]
	1777652795856 -> 1777652796096
	1772112177856 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1772112177856 -> 1777652795856
	1777652795856 [label=AccumulateGrad]
	1777652796432 -> 1777652796384
	1772112169376 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1772112169376 -> 1777652796432
	1777652796432 [label=AccumulateGrad]
	1777652796672 -> 1777652796384
	1772112170016 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1772112170016 -> 1777652796672
	1777652796672 [label=AccumulateGrad]
	1777652796960 -> 1777652797584
	1777653210128 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1777653210128 -> 1777652796960
	1777652796960 [label=AccumulateGrad]
	1777652797536 -> 1777652797824
	1777653206048 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1777653206048 -> 1777652797536
	1777652797536 [label=AccumulateGrad]
	1777652797872 -> 1777652797824
	1777653206528 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1777653206528 -> 1777652797872
	1777652797872 [label=AccumulateGrad]
	1777652798160 -> 1777652798112
	1777652798400 -> 1777652798976
	1777653214608 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1777653214608 -> 1777652798400
	1777652798400 [label=AccumulateGrad]
	1777652799264 -> 1777652799312
	1777653209968 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1777653209968 -> 1777652799264
	1777652799264 [label=AccumulateGrad]
	1777652799552 -> 1777652799312
	1777653216528 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1777653216528 -> 1777652799552
	1777652799552 [label=AccumulateGrad]
	1777652799840 -> 1777652800464
	1777653212368 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1777653212368 -> 1777652799840
	1777652799840 [label=AccumulateGrad]
	1777652800416 -> 1777652800752
	1777653207088 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1777653207088 -> 1777652800416
	1777652800416 [label=AccumulateGrad]
	1777652801040 -> 1777652800752
	1777653203248 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1777653203248 -> 1777652801040
	1777652801040 [label=AccumulateGrad]
	1777652801328 -> 1777652801568
	1772110057440 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1772110057440 -> 1777652801328
	1777652801328 [label=AccumulateGrad]
	1777652801904 -> 1777652802192
	1772110060000 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1772110060000 -> 1777652801904
	1777652801904 [label=AccumulateGrad]
	1777652801856 -> 1777652802192
	1772110061920 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1772110061920 -> 1777652801856
	1777652801856 [label=AccumulateGrad]
	1777652802144 -> 1777652802480
	1777652803056 -> 1777652803296
	1772110059600 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1772110059600 -> 1777652803056
	1777652803056 [label=AccumulateGrad]
	1777652803632 -> 1777652803584
	1772110061200 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1772110061200 -> 1777652803632
	1777652803632 [label=AccumulateGrad]
	1777652803872 -> 1777652803584
	1772110061280 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1772110061280 -> 1777652803872
	1777652803872 [label=AccumulateGrad]
	1777652804160 -> 1777652804784
	1772110059360 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1772110059360 -> 1777652804160
	1777652804160 [label=AccumulateGrad]
	1777652804736 -> 1777652805072
	1772110056640 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1772110056640 -> 1777652804736
	1777652804736 [label=AccumulateGrad]
	1777652805360 -> 1777652805072
	1772110061840 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1772110061840 -> 1777652805360
	1777652805360 [label=AccumulateGrad]
	1777652805648 -> 1777652805888
	1773268977600 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1773268977600 -> 1777652805648
	1777652805648 [label=AccumulateGrad]
	1777652806224 -> 1777652806512
	1773268979600 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1773268979600 -> 1777652806224
	1777652806224 [label=AccumulateGrad]
	1777652806176 -> 1777652806512
	1773268981760 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1773268981760 -> 1777652806176
	1777652806176 [label=AccumulateGrad]
	1777652806464 -> 1777652806800
	1777652806464 [label=CudnnBatchNormBackward0]
	1777652804496 -> 1777652806464
	1777652804496 [label=ConvolutionBackward0]
	1777652802720 -> 1777652804496
	1777652803344 -> 1777652804496
	1772110058160 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1772110058160 -> 1777652803344
	1777652803344 [label=AccumulateGrad]
	1777652805600 -> 1777652806464
	1772110059120 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1772110059120 -> 1777652805600
	1777652805600 [label=AccumulateGrad]
	1777652805936 -> 1777652806464
	1772110056480 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1772110056480 -> 1777652805936
	1777652805936 [label=AccumulateGrad]
	1777652807088 -> 1777652807664
	1773268980800 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1773268980800 -> 1777652807088
	1777652807088 [label=AccumulateGrad]
	1777652807616 -> 1777652807952
	1773268982480 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1773268982480 -> 1777652807616
	1777652807616 [label=AccumulateGrad]
	1777652808240 -> 1777652807952
	1773268978320 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1773268978320 -> 1777652808240
	1777652808240 [label=AccumulateGrad]
	1777652808528 -> 1777652808768
	1773268980240 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1773268980240 -> 1777652808528
	1777652808528 [label=AccumulateGrad]
	1777652809104 -> 1777652809056
	1773268981520 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1773268981520 -> 1777652809104
	1777652809104 [label=AccumulateGrad]
	1777652809344 -> 1777652809056
	1773268981680 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1773268981680 -> 1777652809344
	1777652809344 [label=AccumulateGrad]
	1777652809632 -> 1777652826416
	1773268981040 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1773268981040 -> 1777652809632
	1777652809632 [label=AccumulateGrad]
	1777652804304 -> 1777652826368
	1773268983360 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1773268983360 -> 1777652804304
	1777652804304 [label=AccumulateGrad]
	1777652806368 -> 1777652826368
	1773268980880 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1773268980880 -> 1777652806368
	1777652806368 [label=AccumulateGrad]
	1777652826704 -> 1777652826656
	1777652826944 -> 1777652827520
	1773268983920 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1773268983920 -> 1777652826944
	1777652826944 [label=AccumulateGrad]
	1777652827856 -> 1777652827808
	1773268984480 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1773268984480 -> 1777652827856
	1777652827856 [label=AccumulateGrad]
	1777652828096 -> 1777652827808
	1773268976720 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1773268976720 -> 1777652828096
	1777652828096 [label=AccumulateGrad]
	1777652828384 -> 1777652829008
	1773268983040 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1773268983040 -> 1777652828384
	1777652828384 [label=AccumulateGrad]
	1777652828960 -> 1777652829296
	1773268976080 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1773268976080 -> 1777652828960
	1777652828960 [label=AccumulateGrad]
	1777652829584 -> 1777652829296
	1772112231088 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1772112231088 -> 1777652829584
	1777652829584 [label=AccumulateGrad]
	1777652829872 -> 1777652830112
	1772112228128 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1772112228128 -> 1777652829872
	1777652829872 [label=AccumulateGrad]
	1777652830448 -> 1777652830736
	1772112223568 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1772112223568 -> 1777652830448
	1777652830448 [label=AccumulateGrad]
	1777652830400 -> 1777652830736
	1772112222528 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1772112222528 -> 1777652830400
	1777652830400 [label=AccumulateGrad]
	1777652830688 -> 1777652831024
	1777652831312 -> 1777652831888
	1772112224848 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1772112224848 -> 1777652831312
	1777652831312 [label=AccumulateGrad]
	1777652831840 -> 1777652832176
	1772112230208 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1772112230208 -> 1777652831840
	1777652831840 [label=AccumulateGrad]
	1777652832464 -> 1777652832176
	1772112216208 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1772112216208 -> 1777652832464
	1777652832464 [label=AccumulateGrad]
	1777652832752 -> 1777652832992
	1772112217408 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1772112217408 -> 1777652832752
	1777652832752 [label=AccumulateGrad]
	1777652833328 -> 1777652833280
	1772112229408 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1772112229408 -> 1777652833328
	1777652833328 [label=AccumulateGrad]
	1777652833568 -> 1777652833280
	1772112220208 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1772112220208 -> 1777652833568
	1777652833568 [label=AccumulateGrad]
	1777652833856 -> 1777652834480
	1772112220128 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1772112220128 -> 1777652833856
	1777652833856 [label=AccumulateGrad]
	1777652834432 -> 1777652834720
	1772112219648 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1772112219648 -> 1777652834432
	1777652834432 [label=AccumulateGrad]
	1777652834768 -> 1777652834720
	1772112228688 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1772112228688 -> 1777652834768
	1777652834768 [label=AccumulateGrad]
	1777652835056 -> 1777652835008
	1777652835584 -> 1777652836208
	1772112221408 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1772112221408 -> 1777652835584
	1777652835584 [label=AccumulateGrad]
	1777652836160 -> 1777652836496
	1772112218528 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1772112218528 -> 1777652836160
	1777652836160 [label=AccumulateGrad]
	1777652836784 -> 1777652836496
	1772112222048 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1772112222048 -> 1777652836784
	1777652836784 [label=AccumulateGrad]
	1777652837072 -> 1777652837312
	1772112232208 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772112232208 -> 1777652837072
	1777652837072 [label=AccumulateGrad]
	1777652837648 -> 1777652837600
	1772112226528 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1772112226528 -> 1777652837648
	1777652837648 [label=AccumulateGrad]
	1777652837888 -> 1777652837600
	1772112223888 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1772112223888 -> 1777652837888
	1777652837888 [label=AccumulateGrad]
	1777652838176 -> 1777652838800
	1772112226288 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772112226288 -> 1777652838176
	1777652838176 [label=AccumulateGrad]
	1777652838752 -> 1777652839040
	1772112219568 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1772112219568 -> 1777652838752
	1777652838752 [label=AccumulateGrad]
	1777652839088 -> 1777652839040
	1772112226208 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1772112226208 -> 1777652839088
	1777652839088 [label=AccumulateGrad]
	1777652839376 -> 1777652839328
	1777652839376 [label=CudnnBatchNormBackward0]
	1777652837024 -> 1777652839376
	1777652837024 [label=ConvolutionBackward0]
	1777652835632 -> 1777652837024
	1777652835872 -> 1777652837024
	1772112219328 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1772112219328 -> 1777652835872
	1777652835872 [label=AccumulateGrad]
	1777652838512 -> 1777652839376
	1772112228928 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1772112228928 -> 1777652838512
	1777652838512 [label=AccumulateGrad]
	1777652838464 -> 1777652839376
	1772112230848 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1772112230848 -> 1777652838464
	1777652838464 [label=AccumulateGrad]
	1777652839616 -> 1777652840192
	1772111307744 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1772111307744 -> 1777652839616
	1777652839616 [label=AccumulateGrad]
	1777652840528 -> 1777652840480
	1772111306624 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1772111306624 -> 1777652840528
	1777652840528 [label=AccumulateGrad]
	1777652840768 -> 1777652840480
	1772111304944 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1772111304944 -> 1777652840768
	1777652840768 [label=AccumulateGrad]
	1777652841056 -> 1777652841680
	1772111304064 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772111304064 -> 1777652841056
	1777652841056 [label=AccumulateGrad]
	1777652841632 -> 1772111325808
	1772111302784 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1772111302784 -> 1777652841632
	1777652841632 [label=AccumulateGrad]
	1777652841920 -> 1772111325808
	1772111306544 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1772111306544 -> 1777652841920
	1777652841920 [label=AccumulateGrad]
	1772111324656 -> 1772111315632
	1772111308384 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772111308384 -> 1772111324656
	1772111324656 [label=AccumulateGrad]
	1772111327872 -> 1772111325472
	1772111306464 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1772111306464 -> 1772111327872
	1772111327872 [label=AccumulateGrad]
	1772111327632 -> 1772111325472
	1772111312144 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1772111312144 -> 1772111327632
	1772111327632 [label=AccumulateGrad]
	1772111321056 -> 1772111315776
	1772111324032 -> 1772111329456
	1772111832032 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1772111832032 -> 1772111324032
	1772111324032 [label=AccumulateGrad]
	1772111328400 -> 1772111329168
	1772111828032 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1772111828032 -> 1772111328400
	1772111328400 [label=AccumulateGrad]
	1772111331136 -> 1772111329168
	1772111836032 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1772111836032 -> 1772111331136
	1772111331136 [label=AccumulateGrad]
	1772111317696 -> 1772111329264
	1772111834032 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772111834032 -> 1772111317696
	1772111317696 [label=AccumulateGrad]
	1772111329216 -> 1772111330464
	1772111824112 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1772111824112 -> 1772111329216
	1772111329216 [label=AccumulateGrad]
	1772111330992 -> 1772111330464
	1772111837232 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1772111837232 -> 1772111330992
	1772111330992 [label=AccumulateGrad]
	1772111315152 -> 1772111331088
	1772111823632 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772111823632 -> 1772111315152
	1772111315152 [label=AccumulateGrad]
	1772111330896 -> 1772111315200
	1772111824672 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1772111824672 -> 1772111330896
	1772111330896 [label=AccumulateGrad]
	1772111329120 -> 1772111315200
	1772111832752 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1772111832752 -> 1772111329120
	1772111329120 [label=AccumulateGrad]
	1772111324608 -> 1772111329648
	1772111331040 -> 1772111330752
	1772111829072 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1772111829072 -> 1772111331040
	1772111331040 [label=AccumulateGrad]
	1772111328304 -> 1772111330656
	1772111835152 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1772111835152 -> 1772111328304
	1772111328304 [label=AccumulateGrad]
	1772111329408 -> 1772111330656
	1772111825872 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1772111825872 -> 1772111329408
	1772111329408 [label=AccumulateGrad]
	1772111330416 -> 1772111330704
	1772111833552 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772111833552 -> 1772111330416
	1772111330416 [label=AccumulateGrad]
	1772111330512 -> 1772111330224
	1772111832832 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1772111832832 -> 1772111330512
	1772111330512 [label=AccumulateGrad]
	1772111329696 -> 1772111330224
	1772111833952 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1772111833952 -> 1772111329696
	1772111329696 [label=AccumulateGrad]
	1772111330560 -> 1772111329984
	1772111838432 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772111838432 -> 1772111330560
	1772111330560 [label=AccumulateGrad]
	1772111330368 -> 1772111329888
	1772111836832 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1772111836832 -> 1772111330368
	1772111330368 [label=AccumulateGrad]
	1772111330608 -> 1772111329888
	1772111823712 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1772111823712 -> 1772111330608
	1772111330608 [label=AccumulateGrad]
	1772111330032 -> 1772111328640
	1772111330320 -> 1772111330848
	1772111833312 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1772111833312 -> 1772111330320
	1772111330320 [label=AccumulateGrad]
	1772111324944 -> 1772111321824
	1772111834992 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1772111834992 -> 1772111324944
	1772111324944 [label=AccumulateGrad]
	1772111321776 -> 1772111321824
	1772111836352 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1772111836352 -> 1772111321776
	1772111321776 [label=AccumulateGrad]
	1777653091536 -> 1777653094608
	1772111825072 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772111825072 -> 1777653091536
	1777653091536 [label=AccumulateGrad]
	1777653088608 -> 1777653100992
	1772111824592 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1772111824592 -> 1777653088608
	1777653088608 [label=AccumulateGrad]
	1777653092688 -> 1777653100992
	1772111825712 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1772111825712 -> 1777653092688
	1777653092688 [label=AccumulateGrad]
	1777653094704 -> 1777653094848
	1772111829472 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772111829472 -> 1777653094704
	1777653094704 [label=AccumulateGrad]
	1777653092832 -> 1777653094416
	1772111830192 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1772111830192 -> 1777653092832
	1777653092832 [label=AccumulateGrad]
	1777653094992 -> 1777653094416
	1772111830592 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1772111830592 -> 1777653094992
	1777653094992 [label=AccumulateGrad]
	1777653092736 -> 1772110799408
	1772110804448 -> 1772110800992
	1772111835632 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1772111835632 -> 1772110804448
	1772110804448 [label=AccumulateGrad]
	1772110801184 -> 1772110801040
	1772111834272 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1772111834272 -> 1772110801184
	1772110801184 [label=AccumulateGrad]
	1772110801280 -> 1772110801040
	1772111839072 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1772111839072 -> 1772110801280
	1772110801280 [label=AccumulateGrad]
	1772110805456 -> 1772110805936
	1772112054848 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1772112054848 -> 1772110805456
	1772110805456 [label=AccumulateGrad]
	1772110806128 -> 1772110806416
	1772112061968 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1772112061968 -> 1772110806128
	1772110806128 [label=AccumulateGrad]
	1772110801568 -> 1772110806416
	1772112064048 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1772112064048 -> 1772110801568
	1772110801568 [label=AccumulateGrad]
	1772110804304 -> 1772110802816
	1772112055648 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1772112055648 -> 1772110804304
	1772110804304 [label=AccumulateGrad]
	1772110801808 -> 1772110804208
	1772112059088 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1772112059088 -> 1772110801808
	1772110801808 [label=AccumulateGrad]
	1772110803584 -> 1772110804208
	1772112053088 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1772112053088 -> 1772110803584
	1772110803584 [label=AccumulateGrad]
	1772110794464 -> 1772110801088
	1772110802192 -> 1777652706336
	1772112062608 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1772112062608 -> 1772110802192
	1772110802192 [label=AccumulateGrad]
	1777652704368 -> 1777652706288
	1772112061888 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1772112061888 -> 1777652704368
	1777652704368 [label=AccumulateGrad]
	1772110799264 -> 1777652706288
	1772112061568 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1772112061568 -> 1772110799264
	1772110799264 [label=AccumulateGrad]
	1777652706624 -> 1777652697792
	1772112057648 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1772112057648 -> 1777652706624
	1777652706624 [label=AccumulateGrad]
	1777652698896 -> 1777652709504
	1772112058768 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1772112058768 -> 1777652698896
	1777652698896 [label=AccumulateGrad]
	1777652697744 -> 1777652709504
	1772112057568 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1772112057568 -> 1777652697744
	1777652697744 [label=AccumulateGrad]
	1777652698128 -> 1777652699328
	1772112053888 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1772112053888 -> 1777652698128
	1777652698128 [label=AccumulateGrad]
	1777652698608 -> 1777652696688
	1772112053488 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1772112053488 -> 1777652698608
	1777652698608 [label=AccumulateGrad]
	1777652695872 -> 1777652696688
	1772112052608 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1772112052608 -> 1777652695872
	1777652695872 [label=AccumulateGrad]
	1777652709216 -> 1777652697984
	1777652709216 [label=CudnnBatchNormBackward0]
	1777652705712 -> 1777652709216
	1777652705712 [label=ConvolutionBackward0]
	1772110804400 -> 1777652705712
	1777652705760 -> 1777652705712
	1772112061328 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1772112061328 -> 1777652705760
	1777652705760 [label=AccumulateGrad]
	1777652696352 -> 1777652709216
	1772112053728 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1772112053728 -> 1777652696352
	1777652696352 [label=AccumulateGrad]
	1777652696832 -> 1777652709216
	1772112056208 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1772112056208 -> 1777652696832
	1777652696832 [label=AccumulateGrad]
	1777652700432 -> 1777652703792
	1773228658976 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1773228658976 -> 1777652700432
	1777652700432 [label=AccumulateGrad]
	1777652704656 -> 1777652704704
	1772099833264 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1772099833264 -> 1777652704656
	1777652704656 [label=AccumulateGrad]
	1777652702064 -> 1777652704704
	1772099833184 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1772099833184 -> 1777652702064
	1777652702064 [label=AccumulateGrad]
	1777652702496 -> 1777652702400
	1772099832704 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1772099832704 -> 1777652702496
	1777652702496 [label=AccumulateGrad]
	1777652700960 -> 1777652702352
	1772099832784 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1772099832784 -> 1777652700960
	1777652700960 [label=AccumulateGrad]
	1777652702208 -> 1777652702352
	1772099832624 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1772099832624 -> 1777652702208
	1777652702208 [label=AccumulateGrad]
	1777652702016 -> 1777652702256
	1772099832144 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1772099832144 -> 1777652702016
	1777652702016 [label=AccumulateGrad]
	1777652702304 -> 1777652708448
	1772099831744 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1772099831744 -> 1777652702304
	1777652702304 [label=AccumulateGrad]
	1777652704416 -> 1777652708448
	1772099831664 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1772099831664 -> 1777652704416
	1777652704416 [label=AccumulateGrad]
	1777652708688 -> 1777652702112
	1777652702736 -> 1777652704896
	1772099831584 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1772099831584 -> 1777652702736
	1777652702736 [label=AccumulateGrad]
	1777652701872 -> 1777652702832
	1772099831504 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1772099831504 -> 1777652701872
	1777652701872 [label=AccumulateGrad]
	1777652709696 -> 1777652702832
	1772099831424 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1772099831424 -> 1777652709696
	1777652709696 [label=AccumulateGrad]
	1777652708208 -> 1777652708352
	1772099830864 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1772099830864 -> 1777652708208
	1777652708208 [label=AccumulateGrad]
	1777652707920 -> 1777652706576
	1772099830944 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1772099830944 -> 1777652707920
	1777652707920 [label=AccumulateGrad]
	1777652707440 -> 1777652706576
	1772099830784 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1772099830784 -> 1777652707440
	1777652707440 [label=AccumulateGrad]
	1777652707488 -> 1777652696256
	1772099830304 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1772099830304 -> 1777652707488
	1777652707488 [label=AccumulateGrad]
	1777652701248 -> 1777652697024
	1772099830224 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1772099830224 -> 1777652701248
	1777652701248 [label=AccumulateGrad]
	1777652707152 -> 1777652697024
	1772099830144 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1772099830144 -> 1777652707152
	1777652707152 [label=AccumulateGrad]
	1777652696592 -> 1777652697120
	1777652698176 -> 1777670337776
	1777652698176 [label=TBackward0]
	1777652701104 -> 1777652698176
	1777670306720 [label="fc.weight
 (3, 2048)" fillcolor=lightblue]
	1777670306720 -> 1777652701104
	1777652701104 [label=AccumulateGrad]
	1777670337776 -> 1777652514736
}
