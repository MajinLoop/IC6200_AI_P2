digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1681393222896 [label="
 (1, 3)" fillcolor=darkolivegreen1]
	1680889271728 [label=AddmmBackward0]
	1680889271536 -> 1680889271728
	1681393261520 [label="fc.bias
 (3)" fillcolor=lightblue]
	1681393261520 -> 1680889271536
	1680889271536 [label=AccumulateGrad]
	1680889271248 -> 1680889271728
	1680889271248 [label=ViewBackward0]
	1680889271584 -> 1680889271248
	1680889271584 [label=MeanBackward1]
	1680889271056 -> 1680889271584
	1680889271056 [label=ReluBackward0]
	1680889270480 -> 1680889271056
	1680889270480 [label=AddBackward0]
	1680889270576 -> 1680889270480
	1680889270576 [label=CudnnBatchNormBackward0]
	1680889270864 -> 1680889270576
	1680889270864 [label=ConvolutionBackward0]
	1680889046352 -> 1680889270864
	1680889046352 [label=ReluBackward0]
	1680889046832 -> 1680889046352
	1680889046832 [label=CudnnBatchNormBackward0]
	1680889046928 -> 1680889046832
	1680889046928 [label=ConvolutionBackward0]
	1680889047504 -> 1680889046928
	1680889047504 [label=ReluBackward0]
	1680889047984 -> 1680889047504
	1680889047984 [label=CudnnBatchNormBackward0]
	1680889045728 -> 1680889047984
	1680889045728 [label=ConvolutionBackward0]
	1680889270432 -> 1680889045728
	1680889270432 [label=ReluBackward0]
	1680889048944 -> 1680889270432
	1680889048944 [label=AddBackward0]
	1680889049040 -> 1680889048944
	1680889049040 [label=CudnnBatchNormBackward0]
	1680889046976 -> 1680889049040
	1680889046976 [label=ConvolutionBackward0]
	1680889048656 -> 1680889046976
	1680889048656 [label=ReluBackward0]
	1680889048080 -> 1680889048656
	1680889048080 [label=CudnnBatchNormBackward0]
	1680889048272 -> 1680889048080
	1680889048272 [label=ConvolutionBackward0]
	1680889047552 -> 1680889048272
	1680889047552 [label=ReluBackward0]
	1680889047792 -> 1680889047552
	1680889047792 [label=CudnnBatchNormBackward0]
	1680889047120 -> 1680889047792
	1680889047120 [label=ConvolutionBackward0]
	1680889048992 -> 1680889047120
	1680889048992 [label=ReluBackward0]
	1680889047264 -> 1680889048992
	1680889047264 [label=AddBackward0]
	1680889046592 -> 1680889047264
	1680889046592 [label=CudnnBatchNormBackward0]
	1680889046688 -> 1680889046592
	1680889046688 [label=ConvolutionBackward0]
	1680889045968 -> 1680889046688
	1680889045968 [label=ReluBackward0]
	1680889046160 -> 1680889045968
	1680889046160 [label=CudnnBatchNormBackward0]
	1680889045056 -> 1680889046160
	1680889045056 [label=ConvolutionBackward0]
	1680889045104 -> 1680889045056
	1680889045104 [label=ReluBackward0]
	1680889045824 -> 1680889045104
	1680889045824 [label=CudnnBatchNormBackward0]
	1680889045152 -> 1680889045824
	1680889045152 [label=ConvolutionBackward0]
	1681424579648 -> 1680889045152
	1681424579648 [label=ReluBackward0]
	1680502528944 -> 1681424579648
	1680502528944 [label=AddBackward0]
	1680502527072 -> 1680502528944
	1680502527072 [label=CudnnBatchNormBackward0]
	1680986288432 -> 1680502527072
	1680986288432 [label=ConvolutionBackward0]
	1680986288960 -> 1680986288432
	1680986288960 [label=ReluBackward0]
	1680986290064 -> 1680986288960
	1680986290064 [label=CudnnBatchNormBackward0]
	1680986290352 -> 1680986290064
	1680986290352 [label=ConvolutionBackward0]
	1680986290976 -> 1680986290352
	1680986290976 [label=ReluBackward0]
	1680986291504 -> 1680986290976
	1680986291504 [label=CudnnBatchNormBackward0]
	1680986289440 -> 1680986291504
	1680986289440 [label=ConvolutionBackward0]
	1680502528992 -> 1680986289440
	1680502528992 [label=ReluBackward0]
	1680986290016 -> 1680502528992
	1680986290016 [label=AddBackward0]
	1680986292176 -> 1680986290016
	1680986292176 [label=CudnnBatchNormBackward0]
	1680986291840 -> 1680986292176
	1680986291840 [label=ConvolutionBackward0]
	1680986291744 -> 1680986291840
	1680986291744 [label=ReluBackward0]
	1680986291168 -> 1680986291744
	1680986291168 [label=CudnnBatchNormBackward0]
	1680986291408 -> 1680986291168
	1680986291408 [label=ConvolutionBackward0]
	1680986290640 -> 1680986291408
	1680986290640 [label=ReluBackward0]
	1680986290112 -> 1680986290640
	1680986290112 [label=CudnnBatchNormBackward0]
	1680986290208 -> 1680986290112
	1680986290208 [label=ConvolutionBackward0]
	1680986292128 -> 1680986290208
	1680986292128 [label=ReluBackward0]
	1680986289776 -> 1680986292128
	1680986289776 [label=AddBackward0]
	1680986288672 -> 1680986289776
	1680986288672 [label=CudnnBatchNormBackward0]
	1680986289104 -> 1680986288672
	1680986289104 [label=ConvolutionBackward0]
	1680986288528 -> 1680986289104
	1680986288528 [label=ReluBackward0]
	1680986288720 -> 1680986288528
	1680986288720 [label=CudnnBatchNormBackward0]
	1680986288192 -> 1680986288720
	1680986288192 [label=ConvolutionBackward0]
	1680986289008 -> 1680986288192
	1680986289008 [label=ReluBackward0]
	1680986289392 -> 1680986289008
	1680986289392 [label=CudnnBatchNormBackward0]
	1681316453488 -> 1680986289392
	1681316453488 [label=ConvolutionBackward0]
	1680986289824 -> 1681316453488
	1680986289824 [label=ReluBackward0]
	1680986337968 -> 1680986289824
	1680986337968 [label=AddBackward0]
	1680986338448 -> 1680986337968
	1680986338448 [label=CudnnBatchNormBackward0]
	1680986338928 -> 1680986338448
	1680986338928 [label=ConvolutionBackward0]
	1680986339408 -> 1680986338928
	1680986339408 [label=ReluBackward0]
	1680986339936 -> 1680986339408
	1680986339936 [label=CudnnBatchNormBackward0]
	1680986339984 -> 1680986339936
	1680986339984 [label=ConvolutionBackward0]
	1680986340560 -> 1680986339984
	1680986340560 [label=ReluBackward0]
	1680986341040 -> 1680986340560
	1680986341040 [label=CudnnBatchNormBackward0]
	1680986341088 -> 1680986341040
	1680986341088 [label=ConvolutionBackward0]
	1680986338352 -> 1680986341088
	1680986338352 [label=ReluBackward0]
	1680986341232 -> 1680986338352
	1680986341232 [label=AddBackward0]
	1680986340752 -> 1680986341232
	1680986340752 [label=CudnnBatchNormBackward0]
	1680986340896 -> 1680986340752
	1680986340896 [label=ConvolutionBackward0]
	1680986340320 -> 1680986340896
	1680986340320 [label=ReluBackward0]
	1680986339696 -> 1680986340320
	1680986339696 [label=CudnnBatchNormBackward0]
	1680986339648 -> 1680986339696
	1680986339648 [label=ConvolutionBackward0]
	1680986339072 -> 1680986339648
	1680986339072 [label=ReluBackward0]
	1680986338640 -> 1680986339072
	1680986338640 [label=CudnnBatchNormBackward0]
	1680986338736 -> 1680986338640
	1680986338736 [label=ConvolutionBackward0]
	1680986340272 -> 1680986338736
	1680986340272 [label=ReluBackward0]
	1680986338208 -> 1680986340272
	1680986338208 [label=AddBackward0]
	1680986337584 -> 1680986338208
	1680986337584 [label=CudnnBatchNormBackward0]
	1680986337536 -> 1680986337584
	1680986337536 [label=ConvolutionBackward0]
	1680986340704 -> 1680986337536
	1680986340704 [label=ReluBackward0]
	1680986341280 -> 1680986340704
	1680986341280 [label=CudnnBatchNormBackward0]
	1680986338784 -> 1680986341280
	1680986338784 [label=ConvolutionBackward0]
	1680889092560 -> 1680986338784
	1680889092560 [label=ReluBackward0]
	1680889091552 -> 1680889092560
	1680889091552 [label=CudnnBatchNormBackward0]
	1680889091648 -> 1680889091552
	1680889091648 [label=ConvolutionBackward0]
	1680502471648 -> 1680889091648
	1680502471648 [label=ReluBackward0]
	1680502470784 -> 1680502471648
	1680502470784 [label=AddBackward0]
	1680502469296 -> 1680502470784
	1680502469296 [label=CudnnBatchNormBackward0]
	1680502469056 -> 1680502469296
	1680502469056 [label=ConvolutionBackward0]
	1680502468768 -> 1680502469056
	1680502468768 [label=ReluBackward0]
	1680986346352 -> 1680502468768
	1680986346352 [label=CudnnBatchNormBackward0]
	1680986346448 -> 1680986346352
	1680986346448 [label=ConvolutionBackward0]
	1680986346976 -> 1680986346448
	1680986346976 [label=ReluBackward0]
	1680986347456 -> 1680986346976
	1680986347456 [label=CudnnBatchNormBackward0]
	1680986348464 -> 1680986347456
	1680986348464 [label=ConvolutionBackward0]
	1680502471120 -> 1680986348464
	1680502471120 [label=ReluBackward0]
	1680986349136 -> 1680502471120
	1680986349136 [label=AddBackward0]
	1680986347168 -> 1680986349136
	1680986347168 [label=CudnnBatchNormBackward0]
	1680986347504 -> 1680986347168
	1680986347504 [label=ConvolutionBackward0]
	1680986349376 -> 1680986347504
	1680986349376 [label=ReluBackward0]
	1680986348512 -> 1680986349376
	1680986348512 [label=CudnnBatchNormBackward0]
	1680986348752 -> 1680986348512
	1680986348752 [label=ConvolutionBackward0]
	1680986348896 -> 1680986348752
	1680986348896 [label=ReluBackward0]
	1680986348128 -> 1680986348896
	1680986348128 [label=CudnnBatchNormBackward0]
	1680986347360 -> 1680986348128
	1680986347360 [label=ConvolutionBackward0]
	1680986349520 -> 1680986347360
	1680986349520 [label=ReluBackward0]
	1680986346688 -> 1680986349520
	1680986346688 [label=AddBackward0]
	1680986347216 -> 1680986346688
	1680986347216 [label=CudnnBatchNormBackward0]
	1680986346640 -> 1680986347216
	1680986346640 [label=ConvolutionBackward0]
	1680986346784 -> 1680986346640
	1680986346784 [label=ReluBackward0]
	1680986348368 -> 1680986346784
	1680986348368 [label=CudnnBatchNormBackward0]
	1680986347600 -> 1680986348368
	1680986347600 [label=ConvolutionBackward0]
	1680986346256 -> 1680986347600
	1680986346256 [label=ReluBackward0]
	1680986345680 -> 1680986346256
	1680986345680 [label=CudnnBatchNormBackward0]
	1680986348320 -> 1680986345680
	1680986348320 [label=ConvolutionBackward0]
	1680986347072 -> 1680986348320
	1680986347072 [label=ReluBackward0]
	1680986678464 -> 1680986347072
	1680986678464 [label=AddBackward0]
	1680986679952 -> 1680986678464
	1680986679952 [label=CudnnBatchNormBackward0]
	1680889210336 -> 1680986679952
	1680889210336 [label=ConvolutionBackward0]
	1680889212112 -> 1680889210336
	1680889212112 [label=ReluBackward0]
	1680889212352 -> 1680889212112
	1680889212352 [label=CudnnBatchNormBackward0]
	1680889212544 -> 1680889212352
	1680889212544 [label=ConvolutionBackward0]
	1680889212640 -> 1680889212544
	1680889212640 [label=ReluBackward0]
	1680889211680 -> 1680889212640
	1680889211680 [label=CudnnBatchNormBackward0]
	1680889212064 -> 1680889211680
	1680889212064 [label=ConvolutionBackward0]
	1680889211488 -> 1680889212064
	1680889211488 [label=ReluBackward0]
	1680889209760 -> 1680889211488
	1680889209760 [label=AddBackward0]
	1680889210384 -> 1680889209760
	1680889210384 [label=CudnnBatchNormBackward0]
	1680889209664 -> 1680889210384
	1680889209664 [label=ConvolutionBackward0]
	1680889208992 -> 1680889209664
	1680889208992 [label=ReluBackward0]
	1680889209568 -> 1680889208992
	1680889209568 [label=CudnnBatchNormBackward0]
	1680889209280 -> 1680889209568
	1680889209280 [label=ConvolutionBackward0]
	1680889242912 -> 1680889209280
	1680889242912 [label=ReluBackward0]
	1680889243920 -> 1680889242912
	1680889243920 [label=CudnnBatchNormBackward0]
	1680889244496 -> 1680889243920
	1680889244496 [label=ConvolutionBackward0]
	1680889210432 -> 1680889244496
	1680889210432 [label=ReluBackward0]
	1680889244016 -> 1680889210432
	1680889244016 [label=AddBackward0]
	1680889245264 -> 1680889244016
	1680889245264 [label=CudnnBatchNormBackward0]
	1680889244976 -> 1680889245264
	1680889244976 [label=ConvolutionBackward0]
	1680889243344 -> 1680889244976
	1680889243344 [label=ReluBackward0]
	1680889242336 -> 1680889243344
	1680889242336 [label=CudnnBatchNormBackward0]
	1680502471408 -> 1680889242336
	1680502471408 [label=ConvolutionBackward0]
	1680889273840 -> 1680502471408
	1680889273840 [label=ReluBackward0]
	1680889272976 -> 1680889273840
	1680889272976 [label=CudnnBatchNormBackward0]
	1680889272592 -> 1680889272976
	1680889272592 [label=ConvolutionBackward0]
	1680889245168 -> 1680889272592
	1680889245168 [label=ReluBackward0]
	1680986348416 -> 1680889245168
	1680986348416 [label=AddBackward0]
	1680889211104 -> 1680986348416
	1680889211104 [label=CudnnBatchNormBackward0]
	1680986339888 -> 1680889211104
	1680986339888 [label=ConvolutionBackward0]
	1680889047840 -> 1680986339888
	1680889047840 [label=ReluBackward0]
	1680889046256 -> 1680889047840
	1680889046256 [label=CudnnBatchNormBackward0]
	1680889045200 -> 1680889046256
	1680889045200 [label=ConvolutionBackward0]
	1680986291984 -> 1680889045200
	1680986291984 [label=ReluBackward0]
	1680986303984 -> 1680986291984
	1680986303984 [label=CudnnBatchNormBackward0]
	1680889242144 -> 1680986303984
	1680889242144 [label=ConvolutionBackward0]
	1680889245360 -> 1680889242144
	1680889245360 [label=MaxPool2DWithIndicesBackward0]
	1680889242528 -> 1680889245360
	1680889242528 [label=ReluBackward0]
	1680889244352 -> 1680889242528
	1680889244352 [label=CudnnBatchNormBackward0]
	1680889243248 -> 1680889244352
	1680889243248 [label=ConvolutionBackward0]
	1681400016464 -> 1680889243248
	1681393208352 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1681393208352 -> 1681400016464
	1681400016464 [label=AccumulateGrad]
	1680889245072 -> 1680889244352
	1681393205552 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1681393205552 -> 1680889245072
	1680889245072 [label=AccumulateGrad]
	1681400015840 -> 1680889244352
	1681393409696 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1681393409696 -> 1681400015840
	1681400015840 [label=AccumulateGrad]
	1680889242624 -> 1680889242144
	1681393408736 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1681393408736 -> 1680889242624
	1680889242624 [label=AccumulateGrad]
	1680889242432 -> 1680986303984
	1681393409376 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1681393409376 -> 1680889242432
	1680889242432 [label=AccumulateGrad]
	1680889243488 -> 1680986303984
	1681393409136 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1681393409136 -> 1680889243488
	1680889243488 [label=AccumulateGrad]
	1680986302352 -> 1680889045200
	1680502962512 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1680502962512 -> 1680986302352
	1680986302352 [label=AccumulateGrad]
	1680889047312 -> 1680889046256
	1680502964112 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1680502964112 -> 1680889047312
	1680889047312 [label=AccumulateGrad]
	1680986288288 -> 1680889046256
	1680502962672 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1680502962672 -> 1680986288288
	1680986288288 [label=AccumulateGrad]
	1680889048368 -> 1680986339888
	1680502963552 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1680502963552 -> 1680889048368
	1680889048368 [label=AccumulateGrad]
	1680986337776 -> 1680889211104
	1680502960832 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	1680502960832 -> 1680986337776
	1680986337776 [label=AccumulateGrad]
	1680986338832 -> 1680889211104
	1680502961152 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	1680502961152 -> 1680986338832
	1680986338832 [label=AccumulateGrad]
	1680889209856 -> 1680986348416
	1680889209856 [label=CudnnBatchNormBackward0]
	1680986289344 -> 1680889209856
	1680986289344 [label=ConvolutionBackward0]
	1680889245360 -> 1680986289344
	1680889046784 -> 1680986289344
	1681393409216 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1681393409216 -> 1680889046784
	1680889046784 [label=AccumulateGrad]
	1680986300768 -> 1680889209856
	1681393407456 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1681393407456 -> 1680986300768
	1680986300768 [label=AccumulateGrad]
	1680986340944 -> 1680889209856
	1681393408576 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1681393408576 -> 1680986340944
	1680986340944 [label=AccumulateGrad]
	1680889270672 -> 1680889272592
	1680502960432 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1680502960432 -> 1680889270672
	1680889270672 [label=AccumulateGrad]
	1680889272688 -> 1680889272976
	1680986758976 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1680986758976 -> 1680889272688
	1680889272688 [label=AccumulateGrad]
	1680889273696 -> 1680889272976
	1680986755536 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1680986755536 -> 1680889273696
	1680889273696 [label=AccumulateGrad]
	1680889273984 -> 1680502471408
	1680986757376 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1680986757376 -> 1680889273984
	1680889273984 [label=AccumulateGrad]
	1680889271632 -> 1680889242336
	1680986757776 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1680986757776 -> 1680889271632
	1680889271632 [label=AccumulateGrad]
	1680889271104 -> 1680889242336
	1680986755936 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1680986755936 -> 1680889271104
	1680889271104 [label=AccumulateGrad]
	1680889242576 -> 1680889244976
	1680986757456 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1680986757456 -> 1680889242576
	1680889242576 [label=AccumulateGrad]
	1680889244784 -> 1680889245264
	1680986755136 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	1680986755136 -> 1680889244784
	1680889244784 [label=AccumulateGrad]
	1680889244400 -> 1680889245264
	1680986757696 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	1680986757696 -> 1680889244400
	1680889244400 [label=AccumulateGrad]
	1680889245168 -> 1680889244016
	1680889244112 -> 1680889244496
	1680986757216 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1680986757216 -> 1680889244112
	1680889244112 [label=AccumulateGrad]
	1680889243824 -> 1680889243920
	1680986756336 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	1680986756336 -> 1680889243824
	1680889243824 [label=AccumulateGrad]
	1680889243632 -> 1680889243920
	1680986757136 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	1680986757136 -> 1680889243632
	1680889243632 [label=AccumulateGrad]
	1680889243008 -> 1680889209280
	1681393204096 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1681393204096 -> 1680889243008
	1680889243008 [label=AccumulateGrad]
	1680889241760 -> 1680889209568
	1681393204176 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	1681393204176 -> 1680889241760
	1680889241760 [label=AccumulateGrad]
	1680889241664 -> 1680889209568
	1681393203856 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	1681393203856 -> 1680889241664
	1680889241664 [label=AccumulateGrad]
	1680889209088 -> 1680889209664
	1681393203056 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1681393203056 -> 1680889209088
	1680889209088 [label=AccumulateGrad]
	1680889211008 -> 1680889210384
	1681393205136 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	1681393205136 -> 1680889211008
	1680889211008 [label=AccumulateGrad]
	1680889210624 -> 1680889210384
	1681393202816 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	1681393202816 -> 1680889210624
	1680889210624 [label=AccumulateGrad]
	1680889210432 -> 1680889209760
	1680889210720 -> 1680889212064
	1680986713680 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1680986713680 -> 1680889210720
	1680889210720 [label=AccumulateGrad]
	1680889211584 -> 1680889211680
	1680986711760 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1680986711760 -> 1680889211584
	1680889211584 [label=AccumulateGrad]
	1680889211872 -> 1680889211680
	1680986711040 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1680986711040 -> 1680889211872
	1680889211872 [label=AccumulateGrad]
	1680889212736 -> 1680889212544
	1680986713920 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1680986713920 -> 1680889212736
	1680889212736 [label=AccumulateGrad]
	1680889212256 -> 1680889212352
	1680986713120 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1680986713120 -> 1680889212256
	1680889212256 [label=AccumulateGrad]
	1680889211392 -> 1680889212352
	1680986710400 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1680986710400 -> 1680889211392
	1680889211392 [label=AccumulateGrad]
	1680889211296 -> 1680889210336
	1680986710480 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1680986710480 -> 1680889211296
	1680889211296 [label=AccumulateGrad]
	1680889210144 -> 1680986679952
	1680986710320 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	1680986710320 -> 1680889210144
	1680889210144 [label=AccumulateGrad]
	1680889209520 -> 1680986679952
	1680986712960 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	1680986712960 -> 1680889209520
	1680889209520 [label=AccumulateGrad]
	1680889209376 -> 1680986678464
	1680889209376 [label=CudnnBatchNormBackward0]
	1680889212448 -> 1680889209376
	1680889212448 [label=ConvolutionBackward0]
	1680889211488 -> 1680889212448
	1680889211968 -> 1680889212448
	1679669058672 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1679669058672 -> 1680889211968
	1680889211968 [label=AccumulateGrad]
	1680889211248 -> 1680889209376
	1679669058432 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1679669058432 -> 1680889211248
	1680889211248 [label=AccumulateGrad]
	1680889210240 -> 1680889209376
	1680501386112 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1680501386112 -> 1680889210240
	1680889210240 [label=AccumulateGrad]
	1680986347936 -> 1680986348320
	1680986712720 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1680986712720 -> 1680986347936
	1680986347936 [label=AccumulateGrad]
	1680986345728 -> 1680986345680
	1680986712880 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1680986712880 -> 1680986345728
	1680986345728 [label=AccumulateGrad]
	1680986345584 -> 1680986345680
	1680986710640 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1680986710640 -> 1680986345584
	1680986345584 [label=AccumulateGrad]
	1680986346208 -> 1680986347600
	1681399950208 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1681399950208 -> 1680986346208
	1680986346208 [label=AccumulateGrad]
	1680986346112 -> 1680986348368
	1681399947328 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1681399947328 -> 1680986346112
	1680986346112 [label=AccumulateGrad]
	1680986347552 -> 1680986348368
	1681399949888 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1681399949888 -> 1680986347552
	1680986347552 [label=AccumulateGrad]
	1680986346736 -> 1680986346640
	1681399947408 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1681399947408 -> 1680986346736
	1680986346736 [label=AccumulateGrad]
	1680986346160 -> 1680986347216
	1681399948208 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	1681399948208 -> 1680986346160
	1680986346160 [label=AccumulateGrad]
	1680986347264 -> 1680986347216
	1681399949808 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	1681399949808 -> 1680986347264
	1680986347264 [label=AccumulateGrad]
	1680986347072 -> 1680986346688
	1680986347840 -> 1680986347360
	1681399949008 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1681399949008 -> 1680986347840
	1680986347840 [label=AccumulateGrad]
	1680986348176 -> 1680986348128
	1681399947888 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	1681399947888 -> 1680986348176
	1680986348176 [label=AccumulateGrad]
	1680986347744 -> 1680986348128
	1681399949168 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	1681399949168 -> 1680986347744
	1680986347744 [label=AccumulateGrad]
	1680986348848 -> 1680986348752
	1681399948608 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1681399948608 -> 1680986348848
	1680986348848 [label=AccumulateGrad]
	1680986348272 -> 1680986348512
	1681399949968 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	1681399949968 -> 1680986348272
	1680986348272 [label=AccumulateGrad]
	1680986348032 -> 1680986348512
	1681399947648 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	1681399947648 -> 1680986348032
	1680986348032 [label=AccumulateGrad]
	1680986349232 -> 1680986347504
	1681399950608 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1681399950608 -> 1680986349232
	1680986349232 [label=AccumulateGrad]
	1680986348800 -> 1680986347168
	1681399948528 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	1681399948528 -> 1680986348800
	1680986348800 [label=AccumulateGrad]
	1680986349328 -> 1680986347168
	1681399947728 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	1681399947728 -> 1680986349328
	1680986349328 [label=AccumulateGrad]
	1680986349520 -> 1680986349136
	1680986349040 -> 1680986348464
	1681399994384 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1681399994384 -> 1680986349040
	1680986349040 [label=AccumulateGrad]
	1680986347984 -> 1680986347456
	1681399994064 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	1681399994064 -> 1680986347984
	1680986347984 [label=AccumulateGrad]
	1680986347024 -> 1680986347456
	1681399995904 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	1681399995904 -> 1680986347024
	1680986347024 [label=AccumulateGrad]
	1680986346928 -> 1680986346448
	1681399994224 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1681399994224 -> 1680986346928
	1680986346928 [label=AccumulateGrad]
	1680986346400 -> 1680986346352
	1681399995504 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	1681399995504 -> 1680986346400
	1680986346400 [label=AccumulateGrad]
	1680986345920 -> 1680986346352
	1681399994784 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	1681399994784 -> 1680986345920
	1680986345920 [label=AccumulateGrad]
	1680986345872 -> 1680502469056
	1681399993824 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1681399993824 -> 1680986345872
	1680986345872 [label=AccumulateGrad]
	1680502472512 -> 1680502469296
	1681399993104 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	1681399993104 -> 1680502472512
	1680502472512 [label=AccumulateGrad]
	1680502471840 -> 1680502469296
	1681399993504 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	1681399993504 -> 1680502471840
	1680502471840 [label=AccumulateGrad]
	1680502471120 -> 1680502470784
	1680502469344 -> 1680889091648
	1681399992704 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	1681399992704 -> 1680502469344
	1680502469344 [label=AccumulateGrad]
	1680889091456 -> 1680889091552
	1681399995744 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1681399995744 -> 1680889091456
	1680889091456 [label=AccumulateGrad]
	1680889090784 -> 1680889091552
	1681399996304 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1681399996304 -> 1680889090784
	1680889090784 [label=AccumulateGrad]
	1680889090880 -> 1680986338784
	1681393211888 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1681393211888 -> 1680889090880
	1680889090880 [label=AccumulateGrad]
	1680889093376 -> 1680986341280
	1681393210128 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1681393210128 -> 1680889093376
	1680889093376 [label=AccumulateGrad]
	1680889092608 -> 1680986341280
	1681393211648 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1681393211648 -> 1680889092608
	1680889092608 [label=AccumulateGrad]
	1680986337728 -> 1680986337536
	1681393211328 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1681393211328 -> 1680986337728
	1680986337728 [label=AccumulateGrad]
	1680986338688 -> 1680986337584
	1681393213328 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1681393213328 -> 1680986338688
	1680986338688 [label=AccumulateGrad]
	1680986337488 -> 1680986337584
	1681393209728 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1681393209728 -> 1680986337488
	1680986337488 [label=AccumulateGrad]
	1680986338256 -> 1680986338208
	1680986338256 [label=CudnnBatchNormBackward0]
	1680986340224 -> 1680986338256
	1680986340224 [label=ConvolutionBackward0]
	1680502471648 -> 1680986340224
	1680889091936 -> 1680986340224
	1681399992944 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1681399992944 -> 1680889091936
	1680889091936 [label=AccumulateGrad]
	1680986338160 -> 1680986338256
	1681399992464 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1681399992464 -> 1680986338160
	1680986338160 [label=AccumulateGrad]
	1680986337680 -> 1680986338256
	1681399992544 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1681399992544 -> 1680986337680
	1680986337680 [label=AccumulateGrad]
	1680986338016 -> 1680986338736
	1681393210688 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1681393210688 -> 1680986338016
	1680986338016 [label=AccumulateGrad]
	1680986338544 -> 1680986338640
	1681393213248 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1681393213248 -> 1680986338544
	1680986338544 [label=AccumulateGrad]
	1680986339264 -> 1680986338640
	1681393211568 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1681393211568 -> 1680986339264
	1680986339264 [label=AccumulateGrad]
	1680986339840 -> 1680986339648
	1681424613680 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1681424613680 -> 1680986339840
	1680986339840 [label=AccumulateGrad]
	1680986339600 -> 1680986339696
	1680889203104 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1680889203104 -> 1680986339600
	1680986339600 [label=AccumulateGrad]
	1680986340368 -> 1680986339696
	1681424613600 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1681424613600 -> 1680986340368
	1680986340368 [label=AccumulateGrad]
	1680986340176 -> 1680986340896
	1681424614320 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1681424614320 -> 1680986340176
	1680986340176 [label=AccumulateGrad]
	1680986340848 -> 1680986340752
	1681424613920 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1681424613920 -> 1680986340848
	1680986340848 [label=AccumulateGrad]
	1680986340656 -> 1680986340752
	1681424614240 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1681424614240 -> 1680986340656
	1680986340656 [label=AccumulateGrad]
	1680986340272 -> 1680986341232
	1680986340800 -> 1680986341088
	1681424614640 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1681424614640 -> 1680986340800
	1680986340800 [label=AccumulateGrad]
	1680986340992 -> 1680986341040
	1681424614960 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	1681424614960 -> 1680986340992
	1680986340992 [label=AccumulateGrad]
	1680986340512 -> 1680986341040
	1681424615280 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	1681424615280 -> 1680986340512
	1680986340512 [label=AccumulateGrad]
	1680986340464 -> 1680986339984
	1681424616000 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1681424616000 -> 1680986340464
	1680986340464 [label=AccumulateGrad]
	1680986339504 -> 1680986339936
	1681424615920 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	1681424615920 -> 1680986339504
	1680986339504 [label=AccumulateGrad]
	1680986339360 -> 1680986339936
	1681424615600 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	1681424615600 -> 1680986339360
	1680986339360 [label=AccumulateGrad]
	1680986339024 -> 1680986338928
	1681424616320 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1681424616320 -> 1680986339024
	1680986339024 [label=AccumulateGrad]
	1680986338496 -> 1680986338448
	1681424616640 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	1681424616640 -> 1680986338496
	1680986338496 [label=AccumulateGrad]
	1680986338400 -> 1680986338448
	1681424616560 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	1681424616560 -> 1680986338400
	1680986338400 [label=AccumulateGrad]
	1680986338352 -> 1680986337968
	1680986337872 -> 1681316453488
	1681424617120 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1681424617120 -> 1680986337872
	1680986337872 [label=AccumulateGrad]
	1681316454256 -> 1680986289392
	1681424616960 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	1681424616960 -> 1681316454256
	1681316454256 [label=AccumulateGrad]
	1680986337344 -> 1680986289392
	1681424617280 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	1681424617280 -> 1680986337344
	1680986337344 [label=AccumulateGrad]
	1680986291216 -> 1680986288192
	1680604973392 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1680604973392 -> 1680986291216
	1680986291216 [label=AccumulateGrad]
	1680986288768 -> 1680986288720
	1680604974032 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	1680604974032 -> 1680986288768
	1680986288768 [label=AccumulateGrad]
	1680986288576 -> 1680986288720
	1680604972832 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	1680604972832 -> 1680986288576
	1680986288576 [label=AccumulateGrad]
	1680986288624 -> 1680986289104
	1680604973952 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1680604973952 -> 1680986288624
	1680986288624 [label=AccumulateGrad]
	1680986289056 -> 1680986288672
	1681393192128 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	1681393192128 -> 1680986289056
	1680986289056 [label=AccumulateGrad]
	1680986289152 -> 1680986288672
	1681393190848 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	1681393190848 -> 1680986289152
	1680986289152 [label=AccumulateGrad]
	1680986289824 -> 1680986289776
	1680986289584 -> 1680986290208
	1681393238480 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1681393238480 -> 1680986289584
	1680986289584 [label=AccumulateGrad]
	1680986289728 -> 1680986290112
	1681393241120 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	1681393241120 -> 1680986289728
	1680986289728 [label=AccumulateGrad]
	1680986290784 -> 1680986290112
	1681393239760 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	1681393239760 -> 1680986290784
	1680986290784 [label=AccumulateGrad]
	1680986290592 -> 1680986291408
	1681393239280 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1681393239280 -> 1680986290592
	1680986290592 [label=AccumulateGrad]
	1680986291360 -> 1680986291168
	1681393241840 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	1681393241840 -> 1680986291360
	1680986291360 [label=AccumulateGrad]
	1680986291936 -> 1680986291168
	1681393239120 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	1681393239120 -> 1680986291936
	1680986291936 [label=AccumulateGrad]
	1680986291696 -> 1680986291840
	1680889259728 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1680889259728 -> 1680986291696
	1680986291696 [label=AccumulateGrad]
	1680986290736 -> 1680986292176
	1680889259808 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	1680889259808 -> 1680986290736
	1680986290736 [label=AccumulateGrad]
	1680986290160 -> 1680986292176
	1680889260608 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	1680889260608 -> 1680986290160
	1680986290160 [label=AccumulateGrad]
	1680986292128 -> 1680986290016
	1680986289536 -> 1680986289440
	1680889259488 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	1680889259488 -> 1680986289536
	1680986289536 [label=AccumulateGrad]
	1680986291552 -> 1680986291504
	1680889258848 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	1680889258848 -> 1680986291552
	1680986291552 [label=AccumulateGrad]
	1680986291024 -> 1680986291504
	1680889261328 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	1680889261328 -> 1680986291024
	1680986291024 [label=AccumulateGrad]
	1680986290928 -> 1680986290352
	1680889261568 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1680889261568 -> 1680986290928
	1680986290928 [label=AccumulateGrad]
	1680986290400 -> 1680986290064
	1680889258528 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	1680889258528 -> 1680986290400
	1680986290400 [label=AccumulateGrad]
	1680986289488 -> 1680986290064
	1680889258128 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	1680889258128 -> 1680986289488
	1680986289488 [label=AccumulateGrad]
	1680986288912 -> 1680986288432
	1681393253808 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	1681393253808 -> 1680986288912
	1680986288912 [label=AccumulateGrad]
	1680986288384 -> 1680502527072
	1681393250528 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	1681393250528 -> 1680986288384
	1680986288384 [label=AccumulateGrad]
	1680986288336 -> 1680502527072
	1681393253328 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	1681393253328 -> 1680986288336
	1680986288336 [label=AccumulateGrad]
	1680502528992 -> 1680502528944
	1681424580176 -> 1680889045152
	1681393254048 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	1681393254048 -> 1681424580176
	1681424580176 [label=AccumulateGrad]
	1681424580512 -> 1680889045824
	1681393252128 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1681393252128 -> 1681424580512
	1681424580512 [label=AccumulateGrad]
	1681424579792 -> 1680889045824
	1681393253008 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1681393253008 -> 1681424579792
	1681424579792 [label=AccumulateGrad]
	1680889045632 -> 1680889045056
	1681393250848 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1681393250848 -> 1680889045632
	1680889045632 [label=AccumulateGrad]
	1680889045488 -> 1680889046160
	1681393252848 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1681393252848 -> 1680889045488
	1680889045488 [label=AccumulateGrad]
	1680889046016 -> 1680889046160
	1681393253648 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1681393253648 -> 1680889046016
	1680889046016 [label=AccumulateGrad]
	1680889046064 -> 1680889046688
	1680889238368 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1680889238368 -> 1680889046064
	1680889046064 [label=AccumulateGrad]
	1680889046544 -> 1680889046592
	1680889240688 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	1680889240688 -> 1680889046544
	1680889046544 [label=AccumulateGrad]
	1680889046496 -> 1680889046592
	1680889238048 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	1680889238048 -> 1680889046496
	1680889046496 [label=AccumulateGrad]
	1680889046112 -> 1680889047264
	1680889046112 [label=CudnnBatchNormBackward0]
	1681424577392 -> 1680889046112
	1681424577392 [label=ConvolutionBackward0]
	1681424579648 -> 1681424577392
	1680986290496 -> 1681424577392
	1681393253408 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	1681393253408 -> 1680986290496
	1680986290496 [label=AccumulateGrad]
	1680502526160 -> 1680889046112
	1681393252528 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	1681393252528 -> 1680502526160
	1680502526160 [label=AccumulateGrad]
	1680502530000 -> 1680889046112
	1681393252208 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	1681393252208 -> 1680502530000
	1680502530000 [label=AccumulateGrad]
	1680889047216 -> 1680889047120
	1680889240848 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1680889240848 -> 1680889047216
	1680889047216 [label=AccumulateGrad]
	1680889046640 -> 1680889047792
	1680889238688 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1680889238688 -> 1680889046640
	1680889046640 [label=AccumulateGrad]
	1680889047600 -> 1680889047792
	1680889241008 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1680889241008 -> 1680889047600
	1680889047600 [label=AccumulateGrad]
	1680889047648 -> 1680889048272
	1680889266480 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1680889266480 -> 1680889047648
	1680889047648 [label=AccumulateGrad]
	1680889048128 -> 1680889048080
	1680889239328 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1680889239328 -> 1680889048128
	1680889048128 [label=AccumulateGrad]
	1680889047696 -> 1680889048080
	1680889266800 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1680889266800 -> 1680889047696
	1680889047696 [label=AccumulateGrad]
	1680889048608 -> 1680889046976
	1680889269040 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1680889269040 -> 1680889048608
	1680889048608 [label=AccumulateGrad]
	1680889048752 -> 1680889049040
	1681316507536 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	1681316507536 -> 1680889048752
	1680889048752 [label=AccumulateGrad]
	1680889045536 -> 1680889049040
	1680889266320 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	1680889266320 -> 1680889045536
	1680889045536 [label=AccumulateGrad]
	1680889048992 -> 1680889048944
	1680889048512 -> 1680889045728
	1680889266640 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	1680889266640 -> 1680889048512
	1680889048512 [label=AccumulateGrad]
	1680889048032 -> 1680889047984
	1681393259120 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	1681393259120 -> 1680889048032
	1680889048032 [label=AccumulateGrad]
	1680889047888 -> 1680889047984
	1681393262400 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	1681393262400 -> 1680889047888
	1680889047888 [label=AccumulateGrad]
	1680889047456 -> 1680889046928
	1681393259200 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1681393259200 -> 1680889047456
	1680889047456 [label=AccumulateGrad]
	1680889046880 -> 1680889046832
	1681393262080 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	1681393262080 -> 1680889046880
	1680889046880 [label=AccumulateGrad]
	1680889046400 -> 1680889046832
	1681393262320 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	1681393262320 -> 1680889046400
	1680889046400 [label=AccumulateGrad]
	1680889045872 -> 1680889270864
	1681393258800 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	1681393258800 -> 1680889045872
	1680889045872 [label=AccumulateGrad]
	1680889271296 -> 1680889270576
	1681393259920 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	1681393259920 -> 1680889271296
	1680889271296 [label=AccumulateGrad]
	1680889270624 -> 1680889270576
	1681393259840 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	1681393259840 -> 1680889270624
	1680889270624 [label=AccumulateGrad]
	1680889270432 -> 1680889270480
	1680889271152 -> 1680889271728
	1680889271152 [label=TBackward0]
	1680502528224 -> 1680889271152
	1681424838464 [label="fc.weight
 (3, 2048)" fillcolor=lightblue]
	1681424838464 -> 1680502528224
	1680502528224 [label=AccumulateGrad]
	1680889271728 -> 1681393222896
	dpi=300
}
